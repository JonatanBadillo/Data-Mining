# Series de tiempo y causalidad

# Una serie de tiempo univariada es donde las mediciones se recopilan durante una medida de
# tiempo estÃ¡ndar, que podrÃ­a ser por minuto, hora, dÃ­a, semana, mes, etc. Lo que hace que las
# series temporales sean mÃ¡s problemÃ¡ticas que otros datos es que el orden de las
# observaciones importa. Esta dependencia del orden puede hacer que los mÃ©todos de anÃ¡lisis
# estÃ¡ndar produzcan un sesgo o una variaciÃ³n innecesariamente altos.

# Otro aspecto de las series temporales que a menudo se pasa por alto es la causalidad. SÃ­, no
# queremos confundir correlaciÃ³n con causalidad, pero, en el anÃ¡lisis de series temporales,
# podemos aplicar la tÃ©cnica de causalidad de Granger para determinar si la causalidad,
# estadÃ­sticamente hablando, existe.
# Aplicaremos series de tiempo/tÃ©cnicas economÃ©tricas para identificar
# modelos de pronÃ³stico univariados (incluidos conjuntos), modelos de vectores
# autorregresivos y, finalmente, la causalidad de Granger. 

# es posible que no domines completamente el anÃ¡lisis de series de tiempo, pero
# sabrÃ¡s lo suficiente para realizar un anÃ¡lisis efectivo y comprender las cuestiones
# fundamentales a considerar al construir modelos de series de tiempo y crear modelos
# predictivos (pronÃ³sticos).
# Los siguientes son los temas que se cubrirÃ¡n:
# â€¢ AnÃ¡lisis univariado de series de tiempo
# â€¢ Datos de series de tiempo
# â€¢ Modelado y evaluaciÃ³n




# AnÃ¡lisis univariado de series temporales

# Nos centraremos en dos mÃ©todos para analizar y pronosticar una Ãºnica serie temporal:
# modelos de suavizado exponencial (exponential smoothing) y media mÃ³vil integrada
# autorregresiva (ARIMA, Autoregressive Integrated Moving Average). Comenzaremos
# analizando modelos de suavizado exponencial.
# Al igual que los modelos de media mÃ³vil, los modelos de suavizado exponencial utilizan
# ponderaciones para observaciones pasadas. Pero a diferencia de los modelos de media mÃ³vil, 
# cuanto mÃ¡s reciente es la observaciÃ³n, mÃ¡s peso se le da en relaciÃ³n con las posteriores. Hay
# tres posibles parÃ¡metros de suavizado para estimar: el parÃ¡metro de suavizado general, un
# parÃ¡metro de tendencia y el parÃ¡metro de suavizado estacional. Si no hay tendencia o
# estacionalidad, estos parÃ¡metros se vuelven nulos.

# El parÃ¡metro de suavizado produce un pronÃ³stico (forecast) con la siguiente ecuaciÃ³n:
#   ğ‘Œğ‘¡ + 1 = ğ›¼(ğ‘Œğ‘¡) + (1 âˆ’ ğ›¼)ğ‘Œğ‘¡ âˆ’ 1 + (1 âˆ’ ğ›¼)2ğ‘Œğ‘¡ + â‹¯ , donde 0 < ğ›¼ â‰¤ 1

# En esta ecuaciÃ³n, Yt es el valor en ese momento, T, y alfa (Î±) es el parÃ¡metro de suavizado.
# Los algoritmos optimizan el alfa (y otros parÃ¡metros) minimizando los errores, la suma de
# errores cuadrÃ¡ticos (SSE, Sum of Squared Error) o la mÃ¡xima verosimilitud.
# La ecuaciÃ³n de pronÃ³stico junto con las ecuaciones de tendencia y estacionalidad, si
# corresponde, serÃ¡ la siguiente:
#   â€¢ El pronÃ³stico, donde A es la ecuaciÃ³n de suavizado anterior y h es el nÃºmero de
# perÃ­odos de pronÃ³stico: ğ‘Œğ‘¡ + â„ = ğ´ + â„ğµğ‘¡ + ğ‘†ğ‘¡
# â€¢ La ecuaciÃ³n de tendencia: ğµğ‘¡ = ğ›½(ğ´ğ‘¡ âˆ’ ğ´ğ‘¡ âˆ’ 1) + (1 âˆ’ ğ›½)ğµğ‘¡ âˆ’ 1
# â€¢ La estacionalidad, donde m es el nÃºmero de perÃ­odos estacionales:
#   ğ‘†ğ‘¡ = Î©(ğ‘Œğ‘¡ âˆ’ ğ´ğ‘¡ âˆ’ 1 âˆ’ ğµğ‘¡ âˆ’ 1) + (1 âˆ’ Î©)ğ‘†ğ‘¡ âˆ’ ğ‘š

# Esta ecuaciÃ³n se conoce como mÃ©todo de Holt-Winters. La ecuaciÃ³n de pronÃ³stico es de
# naturaleza aditiva y la tendencia es lineal. El mÃ©todo tambiÃ©n permite la inclusiÃ³n de una
# tendencia amortiguada y una estacionalidad multiplicativa, donde la estacionalidad aumenta
# o disminuye proporcionalmente con el tiempo. Con estos modelos, no tienes que preocuparte
# por el supuesto de estacionariedad como en un modelo ARIMA. La estacionariedad es
# cuando la serie de tiempo tiene una media, varianza y correlaciÃ³n constantes entre todos los
# perÃ­odos de tiempo. Dicho esto, sigue siendo importante comprender los modelos ARIMA,
# ya que habrÃ¡ situaciones en las que tendrÃ¡n el mejor rendimiento.
# Comenzando con el modelo autorregresivo, el valor de Y en el tiempo T es una funciÃ³n lineal
# de los valores anteriores de Y. La fÃ³rmula para un modelo autorregresivo de retraso lag-11
# AR(1) es ğ‘Œğ‘¡ = constante + Î¦ğ‘Œğ‘¡ âˆ’ 1 + ğ¸ğ‘¡. Los supuestos crÃ­ticos para el modelo son los
# siguientes:
#   â€¢ Et denota los errores que se distribuyen de manera idÃ©ntica e independiente con una
# media cero y una varianza constante
# â€¢ Los errores son independientes de Yt
# â€¢ Yt, Yt-1, Yt-n... es estacionario, lo que significa que el valor absoluto de Î¦ es menor
# que uno

# Con una serie temporal estacionaria, se puede examinar la funciÃ³n de autocorrelaciÃ³n (ACF, Autocorrelation Function).
# El ACF de una serie estacionaria proporciona correlaciones entre
# Yt e Yt-h para h = 1, 2...n. Usemos R para crear una serie AR(1) y trazarla:

install.packages("forecast")
library(forecast)
set.seed(1966)

# este comando simula una serie temporal de un modelo autorregresivo de orden 1 (AR(1)).
# especifica el modelo AR(1) con un coeficiente autorregresivo de 0.5.
# order = c(1, 0, 0) indica que el modelo tiene un tÃ©rmino autorregresivo de primer orden (1), sin diferencias (0), y sin tÃ©rminos de media mÃ³vil (0).
# ar = 0.5 especifica el coeficiente del tÃ©rmino autorregresivo.
# n = 200 indica que se deben generar 200 observaciones para la serie temporal.
ar1 <- arima.sim(list(order = c(1, 0, 0), ar = 0.5), n = 200)
# Este comando utiliza la funciÃ³n autoplot del paquete forecast para graficar la serie temporal simulada.
forecast::autoplot(ar1, main = "AR1")


# Ahora, examinemos ACF:
# Calcula la funciÃ³n de autocorrelaciÃ³n (ACF) de la serie temporal ar1 sin generar un grÃ¡fico inmediatamente.
# Utiliza autoplot del paquete forecast para graficar la funciÃ³n de autocorrelaciÃ³n.
forecast::autoplot(acf(ar1, plot = F), main = "ACF of simulated AR1")



# El grÃ¡fico ACF muestra que las correlaciones disminuyen exponencialmente a medida que
# aumenta el Lag (retraso). Las lÃ­neas azules punteadas indican las bandas de confianza de una
# correlaciÃ³n significativa. Cualquier lÃ­nea que se extienda por encima o por debajo de la banda
# mÃ­nima se considera significativa. AdemÃ¡s de ACF, tambiÃ©n deberÃ­amos examinar la funciÃ³n
# de autocorrelaciÃ³n parcial (PACF, Partial Autocorrelation Function). La PACF es una
# correlaciÃ³n condicional, lo que significa que la correlaciÃ³n entre Yt e Yt-h estÃ¡ condicionada
# a las observaciones que se encuentran entre los dos. Una forma de entender esto
# intuitivamente es pensar en un modelo de regresiÃ³n lineal y sus coeficientes. Supongamos
# que tienes Y = ï¢0 + ï¢1X1 versus Y = ï¢0 + ï¢1X1 + ï¢2X2. La relaciÃ³n de X a Y en el primer
# modelo es lineal con un coeficiente, pero en el segundo modelo, el coeficiente serÃ¡ diferente
# debido a que ahora tambiÃ©n se tiene en cuenta la relaciÃ³n entre Y y X2. Ten en cuenta que, en
# el siguiente grÃ¡fico PACF, el valor de autocorrelaciÃ³n parcial en el lag-1 es idÃ©ntico al valor
# de autocorrelaciÃ³n en el lag-1, ya que no se trata de una correlaciÃ³n condicional:

forecast::autoplot(pacf(ar1, plot = F), main = "PACF of simulated AR1")

# Podemos suponer con seguridad que la serie es estacionaria a partir de la apariencia del
# grÃ¡fico de la serie temporal anterior. Veremos un par de pruebas estadÃ­sticas en el ejercicio
# prÃ¡ctico para asegurarnos de que los datos sean estacionarios pero, en ocasiones, la prueba
# del globo ocular (eyeball test) es suficiente. Si los datos no son estacionarios, entonces es
# posible eliminar la tendencia de los datos tomando sus diferencias. 

# Este es el Integrado (I) en ARIMA. DespuÃ©s de diferenciar, la nueva serie se convierte en
# Î”Yt = Yt - Yt-1. Se deberÃ­a esperar que una diferencia de primer orden alcance la 
# estacionariedad pero, en algunas ocasiones, puede ser necesaria una diferencia de segundo
# orden. Un modelo ARIMA con AR(1) e I(1) se anotarÃ­a como (1, 1, 0).
# MA significa media mÃ³vil (Moving Average). Este no es el promedio mÃ³vil simple como el
# promedio mÃ³vil de 50 dÃ­as del precio de una acciÃ³n, sino mÃ¡s bien un coeficiente que se
# aplica a los errores. Por supuesto, los errores se distribuyen de forma idÃ©ntica e independiente
# con una media cero y una varianza constante. La fÃ³rmula para un modelo MA(1) es Yt =
# constante + Et + Î˜Et-1. Como hicimos con el modelo AR(1), podemos construir un MA(1)
# en R, de la siguiente manera:

set.seed(123)
ma1 <- arima.sim(list(order = c(0, 0, 1), ma = -0.5), n = 200)
forecast::autoplot(ma1, main = "MA1")

# Los grÃ¡ficos ACF y PACF son un poco diferentes del modelo AR(1). Ten en cuenta que
# existen algunas reglas generales al observar los grÃ¡ficos para determinar si el modelo tiene
# tÃ©rminos AR y/o MA. Pueden ser un poco subjetivos, asÃ­ que dejaremos que tu aprendas estas
# heurÃ­sticas, pero confiamos en R para identificar el modelo adecuado. En los siguientes
# grÃ¡ficos, veremos una correlaciÃ³n significativa en el retraso 1 y dos correlaciones parciales
# significativas en el retraso 1 y el retraso 2:
forecast::autoplot(acf(ma1, plot = F), main = "ACF of simulated MA1")

# La figura anterior es el grÃ¡fico ACF y ahora veremos el grÃ¡fico PACF:
forecast::autoplot(pacf(ma1, plot = F), main = "PACF of simulated MA1")

# Con los modelos ARIMA es posible incorporar la estacionalidad, incluidos los tÃ©rminos
# autorregresivos, integrados y de media mÃ³vil. La notaciÃ³n del modelo ARIMA no estacional
# suele ser (p, d, q). Con ARIMA estacional, supongamos que los datos son mensuales,
# entonces la notaciÃ³n serÃ­a (p, d, q) Ã— (P, D, Q)12, y el 12 en la notaciÃ³n tomarÃ­a en cuenta la
# estacionalidad mensual. En los paquetes que usaremos, R puede identificar automÃ¡ticamente
# si se debe incluir la estacionalidad; de ser asÃ­, tambiÃ©n se incluirÃ¡n los tÃ©rminos Ã³ptimos.



# Comprender la causalidad de Granger
# Imagina que te hacen una pregunta como: Â¿CuÃ¡l es la relaciÃ³n entre el nÃºmero de recetas
# nuevas y el total de recetas del medicamento X? Tu sabes que estos se miden mensualmente,
# entonces, Â¿quÃ© podrÃ­as hacer para comprender esa relaciÃ³n, dado que la gente cree que los
# nuevos guiones aumentarÃ¡n el total de guiones? Â¿O quÃ© tal si se prueba la hipÃ³tesis de que
# los precios de las materias primas (en particular, el cobre) son un indicador adelantado de los
# precios del mercado de valores en Estados Unidos?
#   Bueno, con dos conjuntos de datos de series temporales, x e y, la causalidad de Granger es
# un mÃ©todo que intenta determinar si es probable que una serie influya en un cambio en la
# otra. Esto se hace tomando diferentes rezagos de una serie y utilizÃ¡ndolos para modelar el
# cambio en la segunda serie. Para lograr esto, crearemos dos modelos que predecirÃ¡n y, uno
# solo con los valores pasados de y(Î©) y el otro con los valores pasados de y y x(Ï€). Los
# modelos son los siguientes, donde k es el nÃºmero de rezagos en la serie de tiempo:
#   Sea Î© = ğ‘¦ğ‘¡ = ğ›½0 + ğ›½1ğ‘¦ğ‘¡ âˆ’ 1 + â‹¯ ğ›½ğ‘˜ğ‘¦ğ‘¡ âˆ’ ğ‘˜ + ğœ– y
# sea ğœ‹ = ğ‘¦ğ‘¡ = ğ›½0 + ğ›½1ğ‘¦ğ‘¡ âˆ’ 1 + â‹¯ + ğ›½ğ‘˜ğ‘¦ğ‘¡ âˆ’ ğ‘˜ + ğ›¼1ğ‘¦ğ‘¡ âˆ’ 1 + â‹¯ + ğ›¼ğ‘˜ğ‘¦ğ‘¡ âˆ’ ğ‘˜ + ğœ–
# Luego se compara el RSS y se utiliza la prueba F para determinar si el modelo anidado (Î©)
# es lo suficientemente adecuado para explicar los valores futuros de y o si el modelo completo
# (Ï€) es mejor. La prueba F se utiliza para probar las siguientes hipÃ³tesis nulas y alternativas:
#   â€¢ H0:ï¡1=0 para cada iïƒ[1, k], sin causalidad de Granger
# â€¢ H1: ï¡1â‰ 0 para al menos un iïƒ[1, k], causalidad de Granger

# BÃ¡sicamente, estamos tratando de determinar si podemos decir que, estadÃ­sticamente, x
# proporciona mÃ¡s informaciÃ³n sobre los valores futuros de y que los valores pasados de y
# solos. En esta definiciÃ³n, estÃ¡ claro que no estamos tratando de probar una causalidad real,
# sÃ³lo que los dos valores estÃ¡n relacionados por algÃºn fenÃ³meno. En este sentido, tambiÃ©n
# debemos ejecutar este modelo a la inversa para verificar que y no proporciona informaciÃ³n
# sobre los valores futuros de x. Si encontramos que este es el caso, es probable que haya alguna
# variable exÃ³gena, digamos Z, que deba controlarse o que posiblemente sea una mejor
# candidata para la causalidad de Granger. Originalmente, habÃ­a que aplicar el mÃ©todo a series
# temporales estacionarias para evitar resultados espurios. Este ya no es el caso como se
# demostrarÃ¡.

# Hay un par de formas diferentes de identificar la estructura de retraso (lag) adecuada.
# Naturalmente, podemos utilizar la fuerza bruta y la ignorancia para probar todos los retrasos
# razonables, uno a la vez. Es posible que tengamos una intuiciÃ³n racional basada en la
# experiencia en el dominio o quizÃ¡s en investigaciones previas que existan para guiar la
# selecciÃ³n del retraso.
# De lo contrario, puedes aplicar auto regresiÃ³n vectorial (VAR, Vector Autoregression) para
# identificar la estructura de retraso con el criterio de informaciÃ³n mÃ¡s bajo, como el criterio
# de informaciÃ³n de Aikake (AIC, Aikakeâ€™s Information Criterion) o el error de predicciÃ³n
# final (FPE, Final Prediction Error). Para simplificar, aquÃ­ estÃ¡ la notaciÃ³n para los modelos
# VAR con dos variables, y esto incorpora solo un lag para cada variable. Esta notaciÃ³n se
# puede ampliar para tantas variables y lags como sea apropiado:
#   â€¢ Y = constante1 + B11Yt-1 + B12Yt-1 + e1
# â€¢ X = constante1 + B21Yt-1 + B22Yt-1 + e2
# En R, este proceso es bastante sencillo de implementar como veremos en el siguiente
# problema prÃ¡ctico.



# ------------------------------------------------------------------------------

# Datos de series de tiempo

# El cambio climÃ¡tico estÃ¡ ocurriendo. Siempre lo ha sido y lo serÃ¡, pero la gran pregunta, al
# menos desde un punto de vista polÃ­tico y econÃ³mico, Â¿es el cambio climÃ¡tico provocado por
# el hombre? Utilizaremos esta parte del documento para poner a prueba el modelado
# economÃ©trico de series temporales para tratar de aprender si las emisiones de carbono causan,
# estadÃ­sticamente hablando, el cambio climÃ¡tico y, en particular, el aumento de las
# temperaturas.

# Los datos que usaremos se proporcionan como una anomalÃ­a anual, que se calcula como la
# diferencia de la temperatura superficial anual media para un perÃ­odo de tiempo determinado
# versus el promedio de los aÃ±os de referencia (1961-1990). La temperatura superficial anual
# es un conjunto de temperaturas recopiladas a nivel mundial y combinadas a partir de los
# conjuntos de datos de temperatura del aire en la superficie CRUTEM4 y de la superficie del
# mar HadSST3. Los escÃ©pticos han atacado a los parciales y poco fiables:
#   https://www.telegraph.co.uk/comment/11561629/Top-scientists-start-to-examine-fiddledglobal-warming-figures.html. Esto estÃ¡ muy fuera de nuestro alcance de esfuerzo aquÃ­, por
# lo que debemos aceptar y utilizar estos datos tal como estÃ¡n, pero de todos modos lo
# encontramos divertido. Obtuvimos los datos desde 1919 hasta 2013 para que coincidan con
# nuestros datos de CO2.

# Las estimaciones de emisiones globales de CO2 se pueden encontrar en el Centro de AnÃ¡lisis
# de InformaciÃ³n sobre DiÃ³xido de Carbono (CDIAC) del Departamento de EnergÃ­a de EE.
# UU. en el siguiente sitio web: https://data.globalchange.gov/organization/carbon-dioxideinformation-analysis-center.
# Instalemos bibliotecas segÃºn sea necesario, carguemos los datos y examinemos la estructura:
library(magrittr)
library(ggthemes)
library(tseries)
library(ggplot2)
library(tidyverse)
climate <- readr::read_csv("climate.csv")
str(climate)


# Pondremos esto en una estructura de serie temporal, especificando los aÃ±os de inicio y
# finalizaciÃ³n:
climate_ts <- ts(climate[, 2:3],
                 start = 1919,
                 end = 2013)
#   Con nuestros datos cargados y colocados en estructuras de series temporales, ahora podemos
# comenzar a comprenderlos y prepararlos aÃºn mÃ¡s para el anÃ¡lisis.


# ExploraciÃ³n de datos
# Comencemos con una grÃ¡fica de la serie temporal usando la base de R:

plot(climate_ts, main = "CO2 and Temperature Deviation")

# Parece que los niveles de CO2 realmente comenzaron a aumentar despuÃ©s de la Segunda
# Guerra Mundial y hay un rÃ¡pido aumento de las anomalÃ­as de temperatura a mediados de los
# aÃ±os 1970. No parece haber valores atÃ­picos obvios y la variaciÃ³n a lo largo del tiempo parece
# constante.
# Utilizando el procedimiento estÃ¡ndar, podemos ver que las dos series estÃ¡n altamente
# correlacionadas, como sigue:
cor(climate_ts)
