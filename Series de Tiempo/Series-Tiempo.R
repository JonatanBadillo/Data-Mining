# Series de tiempo y causalidad

# Una serie de tiempo univariada es donde las mediciones se recopilan durante una medida de
# tiempo estÃ¡ndar, que podrÃ­a ser por minuto, hora, dÃ­a, semana, mes, etc. Lo que hace que las
# series temporales sean mÃ¡s problemÃ¡ticas que otros datos es que el orden de las
# observaciones importa. Esta dependencia del orden puede hacer que los mÃ©todos de anÃ¡lisis
# estÃ¡ndar produzcan un sesgo o una variaciÃ³n innecesariamente altos.

# Otro aspecto de las series temporales que a menudo se pasa por alto es la causalidad. SÃ­, no
# queremos confundir correlaciÃ³n con causalidad, pero, en el anÃ¡lisis de series temporales,
# podemos aplicar la tÃ©cnica de causalidad de Granger para determinar si la causalidad,
# estadÃ­sticamente hablando, existe.
# Aplicaremos series de tiempo/tÃ©cnicas economÃ©tricas para identificar
# modelos de pronÃ³stico univariados (incluidos conjuntos), modelos de vectores
# autorregresivos y, finalmente, la causalidad de Granger. 

# es posible que no domines completamente el anÃ¡lisis de series de tiempo, pero
# sabrÃ¡s lo suficiente para realizar un anÃ¡lisis efectivo y comprender las cuestiones
# fundamentales a considerar al construir modelos de series de tiempo y crear modelos
# predictivos (pronÃ³sticos).
# Los siguientes son los temas que se cubrirÃ¡n:
# â€¢ AnÃ¡lisis univariado de series de tiempo
# â€¢ Datos de series de tiempo
# â€¢ Modelado y evaluaciÃ³n




# AnÃ¡lisis univariado de series temporales

# Nos centraremos en dos mÃ©todos para analizar y pronosticar una Ãºnica serie temporal:
# modelos de suavizado exponencial (exponential smoothing) y media mÃ³vil integrada
# autorregresiva (ARIMA, Autoregressive Integrated Moving Average). Comenzaremos
# analizando modelos de suavizado exponencial.
# Al igual que los modelos de media mÃ³vil, los modelos de suavizado exponencial utilizan
# ponderaciones para observaciones pasadas. Pero a diferencia de los modelos de media mÃ³vil, 
# cuanto mÃ¡s reciente es la observaciÃ³n, mÃ¡s peso se le da en relaciÃ³n con las posteriores. Hay
# tres posibles parÃ¡metros de suavizado para estimar: el parÃ¡metro de suavizado general, un
# parÃ¡metro de tendencia y el parÃ¡metro de suavizado estacional. Si no hay tendencia o
# estacionalidad, estos parÃ¡metros se vuelven nulos.

# El parÃ¡metro de suavizado produce un pronÃ³stico (forecast) con la siguiente ecuaciÃ³n:
#   ğ‘Œğ‘¡ + 1 = ğ›¼(ğ‘Œğ‘¡) + (1 âˆ’ ğ›¼)ğ‘Œğ‘¡ âˆ’ 1 + (1 âˆ’ ğ›¼)2ğ‘Œğ‘¡ + â‹¯ , donde 0 < ğ›¼ â‰¤ 1

# En esta ecuaciÃ³n, Yt es el valor en ese momento, T, y alfa (Î±) es el parÃ¡metro de suavizado.
# Los algoritmos optimizan el alfa (y otros parÃ¡metros) minimizando los errores, la suma de
# errores cuadrÃ¡ticos (SSE, Sum of Squared Error) o la mÃ¡xima verosimilitud.
# La ecuaciÃ³n de pronÃ³stico junto con las ecuaciones de tendencia y estacionalidad, si
# corresponde, serÃ¡ la siguiente:
#   â€¢ El pronÃ³stico, donde A es la ecuaciÃ³n de suavizado anterior y h es el nÃºmero de
# perÃ­odos de pronÃ³stico: ğ‘Œğ‘¡ + â„ = ğ´ + â„ğµğ‘¡ + ğ‘†ğ‘¡
# â€¢ La ecuaciÃ³n de tendencia: ğµğ‘¡ = ğ›½(ğ´ğ‘¡ âˆ’ ğ´ğ‘¡ âˆ’ 1) + (1 âˆ’ ğ›½)ğµğ‘¡ âˆ’ 1
# â€¢ La estacionalidad, donde m es el nÃºmero de perÃ­odos estacionales:
#   ğ‘†ğ‘¡ = Î©(ğ‘Œğ‘¡ âˆ’ ğ´ğ‘¡ âˆ’ 1 âˆ’ ğµğ‘¡ âˆ’ 1) + (1 âˆ’ Î©)ğ‘†ğ‘¡ âˆ’ ğ‘š

# Esta ecuaciÃ³n se conoce como mÃ©todo de Holt-Winters. La ecuaciÃ³n de pronÃ³stico es de
# naturaleza aditiva y la tendencia es lineal. El mÃ©todo tambiÃ©n permite la inclusiÃ³n de una
# tendencia amortiguada y una estacionalidad multiplicativa, donde la estacionalidad aumenta
# o disminuye proporcionalmente con el tiempo. Con estos modelos, no tienes que preocuparte
# por el supuesto de estacionariedad como en un modelo ARIMA. La estacionariedad es
# cuando la serie de tiempo tiene una media, varianza y correlaciÃ³n constantes entre todos los
# perÃ­odos de tiempo. Dicho esto, sigue siendo importante comprender los modelos ARIMA,
# ya que habrÃ¡ situaciones en las que tendrÃ¡n el mejor rendimiento.
# Comenzando con el modelo autorregresivo, el valor de Y en el tiempo T es una funciÃ³n lineal
# de los valores anteriores de Y. La fÃ³rmula para un modelo autorregresivo de retraso lag-11
# AR(1) es ğ‘Œğ‘¡ = constante + Î¦ğ‘Œğ‘¡ âˆ’ 1 + ğ¸ğ‘¡. Los supuestos crÃ­ticos para el modelo son los
# siguientes:
#   â€¢ Et denota los errores que se distribuyen de manera idÃ©ntica e independiente con una
# media cero y una varianza constante
# â€¢ Los errores son independientes de Yt
# â€¢ Yt, Yt-1, Yt-n... es estacionario, lo que significa que el valor absoluto de Î¦ es menor
# que uno

# Con una serie temporal estacionaria, se puede examinar la funciÃ³n de autocorrelaciÃ³n (ACF, Autocorrelation Function).
# El ACF de una serie estacionaria proporciona correlaciones entre
# Yt e Yt-h para h = 1, 2...n. Usemos R para crear una serie AR(1) y trazarla:

install.packages("forecast")
library(forecast)
set.seed(1966)

# este comando simula una serie temporal de un modelo autorregresivo de orden 1 (AR(1)).
# especifica el modelo AR(1) con un coeficiente autorregresivo de 0.5.
# order = c(1, 0, 0) indica que el modelo tiene un tÃ©rmino autorregresivo de primer orden (1), sin diferencias (0), y sin tÃ©rminos de media mÃ³vil (0).
# ar = 0.5 especifica el coeficiente del tÃ©rmino autorregresivo.
# n = 200 indica que se deben generar 200 observaciones para la serie temporal.
ar1 <- arima.sim(list(order = c(1, 0, 0), ar = 0.5), n = 200)
# Este comando utiliza la funciÃ³n autoplot del paquete forecast para graficar la serie temporal simulada.
forecast::autoplot(ar1, main = "AR1")


# Ahora, examinemos ACF:
# Calcula la funciÃ³n de autocorrelaciÃ³n (ACF) de la serie temporal ar1 sin generar un grÃ¡fico inmediatamente.
# Utiliza autoplot del paquete forecast para graficar la funciÃ³n de autocorrelaciÃ³n.
forecast::autoplot(acf(ar1, plot = F), main = "ACF of simulated AR1")



# El grÃ¡fico ACF muestra que las correlaciones disminuyen exponencialmente a medida que
# aumenta el Lag (retraso). Las lÃ­neas azules punteadas indican las bandas de confianza de una
# correlaciÃ³n significativa. Cualquier lÃ­nea que se extienda por encima o por debajo de la banda
# mÃ­nima se considera significativa. AdemÃ¡s de ACF, tambiÃ©n deberÃ­amos examinar la funciÃ³n
# de autocorrelaciÃ³n parcial (PACF, Partial Autocorrelation Function). La PACF es una
# correlaciÃ³n condicional, lo que significa que la correlaciÃ³n entre Yt e Yt-h estÃ¡ condicionada
# a las observaciones que se encuentran entre los dos. Una forma de entender esto
# intuitivamente es pensar en un modelo de regresiÃ³n lineal y sus coeficientes. Supongamos
# que tienes Y = ï¢0 + ï¢1X1 versus Y = ï¢0 + ï¢1X1 + ï¢2X2. La relaciÃ³n de X a Y en el primer
# modelo es lineal con un coeficiente, pero en el segundo modelo, el coeficiente serÃ¡ diferente
# debido a que ahora tambiÃ©n se tiene en cuenta la relaciÃ³n entre Y y X2. Ten en cuenta que, en
# el siguiente grÃ¡fico PACF, el valor de autocorrelaciÃ³n parcial en el lag-1 es idÃ©ntico al valor
# de autocorrelaciÃ³n en el lag-1, ya que no se trata de una correlaciÃ³n condicional:

forecast::autoplot(pacf(ar1, plot = F), main = "PACF of simulated AR1")

# Podemos suponer con seguridad que la serie es estacionaria a partir de la apariencia del
# grÃ¡fico de la serie temporal anterior. Veremos un par de pruebas estadÃ­sticas en el ejercicio
# prÃ¡ctico para asegurarnos de que los datos sean estacionarios pero, en ocasiones, la prueba
# del globo ocular (eyeball test) es suficiente. Si los datos no son estacionarios, entonces es
# posible eliminar la tendencia de los datos tomando sus diferencias. 








