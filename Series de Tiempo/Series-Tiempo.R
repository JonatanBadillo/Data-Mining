# Series de tiempo y causalidad

# Una serie de tiempo univariada es donde las mediciones se recopilan durante una medida de
# tiempo est√°ndar, que podr√≠a ser por minuto, hora, d√≠a, semana, mes, etc. Lo que hace que las
# series temporales sean m√°s problem√°ticas que otros datos es que el orden de las
# observaciones importa. Esta dependencia del orden puede hacer que los m√©todos de an√°lisis
# est√°ndar produzcan un sesgo o una variaci√≥n innecesariamente altos.

# Otro aspecto de las series temporales que a menudo se pasa por alto es la causalidad. S√≠, no
# queremos confundir correlaci√≥n con causalidad, pero, en el an√°lisis de series temporales,
# podemos aplicar la t√©cnica de causalidad de Granger para determinar si la causalidad,
# estad√≠sticamente hablando, existe.
# Aplicaremos series de tiempo/t√©cnicas econom√©tricas para identificar
# modelos de pron√≥stico univariados (incluidos conjuntos), modelos de vectores
# autorregresivos y, finalmente, la causalidad de Granger. 

# es posible que no domines completamente el an√°lisis de series de tiempo, pero
# sabr√°s lo suficiente para realizar un an√°lisis efectivo y comprender las cuestiones
# fundamentales a considerar al construir modelos de series de tiempo y crear modelos
# predictivos (pron√≥sticos).
# Los siguientes son los temas que se cubrir√°n:
# ‚Ä¢ An√°lisis univariado de series de tiempo
# ‚Ä¢ Datos de series de tiempo
# ‚Ä¢ Modelado y evaluaci√≥n




# An√°lisis univariado de series temporales

# Nos centraremos en dos m√©todos para analizar y pronosticar una √∫nica serie temporal:
# modelos de suavizado exponencial (exponential smoothing) y media m√≥vil integrada
# autorregresiva (ARIMA, Autoregressive Integrated Moving Average). Comenzaremos
# analizando modelos de suavizado exponencial.
# Al igual que los modelos de media m√≥vil, los modelos de suavizado exponencial utilizan
# ponderaciones para observaciones pasadas. Pero a diferencia de los modelos de media m√≥vil, 
# cuanto m√°s reciente es la observaci√≥n, m√°s peso se le da en relaci√≥n con las posteriores. Hay
# tres posibles par√°metros de suavizado para estimar: el par√°metro de suavizado general, un
# par√°metro de tendencia y el par√°metro de suavizado estacional. Si no hay tendencia o
# estacionalidad, estos par√°metros se vuelven nulos.

# El par√°metro de suavizado produce un pron√≥stico (forecast) con la siguiente ecuaci√≥n:
#   ùëåùë° + 1 = ùõº(ùëåùë°) + (1 ‚àí ùõº)ùëåùë° ‚àí 1 + (1 ‚àí ùõº)2ùëåùë° + ‚ãØ , donde 0 < ùõº ‚â§ 1

# En esta ecuaci√≥n, Yt es el valor en ese momento, T, y alfa (Œ±) es el par√°metro de suavizado.
# Los algoritmos optimizan el alfa (y otros par√°metros) minimizando los errores, la suma de
# errores cuadr√°ticos (SSE, Sum of Squared Error) o la m√°xima verosimilitud.
# La ecuaci√≥n de pron√≥stico junto con las ecuaciones de tendencia y estacionalidad, si
# corresponde, ser√° la siguiente:
#   ‚Ä¢ El pron√≥stico, donde A es la ecuaci√≥n de suavizado anterior y h es el n√∫mero de
# per√≠odos de pron√≥stico: ùëåùë° + ‚Ñé = ùê¥ + ‚Ñéùêµùë° + ùëÜùë°
# ‚Ä¢ La ecuaci√≥n de tendencia: ùêµùë° = ùõΩ(ùê¥ùë° ‚àí ùê¥ùë° ‚àí 1) + (1 ‚àí ùõΩ)ùêµùë° ‚àí 1
# ‚Ä¢ La estacionalidad, donde m es el n√∫mero de per√≠odos estacionales:
#   ùëÜùë° = Œ©(ùëåùë° ‚àí ùê¥ùë° ‚àí 1 ‚àí ùêµùë° ‚àí 1) + (1 ‚àí Œ©)ùëÜùë° ‚àí ùëö

# Esta ecuaci√≥n se conoce como m√©todo de Holt-Winters. La ecuaci√≥n de pron√≥stico es de
# naturaleza aditiva y la tendencia es lineal. El m√©todo tambi√©n permite la inclusi√≥n de una
# tendencia amortiguada y una estacionalidad multiplicativa, donde la estacionalidad aumenta
# o disminuye proporcionalmente con el tiempo. Con estos modelos, no tienes que preocuparte
# por el supuesto de estacionariedad como en un modelo ARIMA. La estacionariedad es
# cuando la serie de tiempo tiene una media, varianza y correlaci√≥n constantes entre todos los
# per√≠odos de tiempo. Dicho esto, sigue siendo importante comprender los modelos ARIMA,
# ya que habr√° situaciones en las que tendr√°n el mejor rendimiento.
# Comenzando con el modelo autorregresivo, el valor de Y en el tiempo T es una funci√≥n lineal
# de los valores anteriores de Y. La f√≥rmula para un modelo autorregresivo de retraso lag-11
# AR(1) es ùëåùë° = constante + Œ¶ùëåùë° ‚àí 1 + ùê∏ùë°. Los supuestos cr√≠ticos para el modelo son los
# siguientes:
#   ‚Ä¢ Et denota los errores que se distribuyen de manera id√©ntica e independiente con una
# media cero y una varianza constante
# ‚Ä¢ Los errores son independientes de Yt
# ‚Ä¢ Yt, Yt-1, Yt-n... es estacionario, lo que significa que el valor absoluto de Œ¶ es menor
# que uno

# Con una serie temporal estacionaria, se puede examinar la funci√≥n de autocorrelaci√≥n (ACF, Autocorrelation Function).
# El ACF de una serie estacionaria proporciona correlaciones entre
# Yt e Yt-h para h = 1, 2...n. Usemos R para crear una serie AR(1) y trazarla:

install.packages("forecast")
library(forecast)
set.seed(1966)

# este comando simula una serie temporal de un modelo autorregresivo de orden 1 (AR(1)).
# especifica el modelo AR(1) con un coeficiente autorregresivo de 0.5.
# order = c(1, 0, 0) indica que el modelo tiene un t√©rmino autorregresivo de primer orden (1), sin diferencias (0), y sin t√©rminos de media m√≥vil (0).
# ar = 0.5 especifica el coeficiente del t√©rmino autorregresivo.
# n = 200 indica que se deben generar 200 observaciones para la serie temporal.
ar1 <- arima.sim(list(order = c(1, 0, 0), ar = 0.5), n = 200)
# Este comando utiliza la funci√≥n autoplot del paquete forecast para graficar la serie temporal simulada.
forecast::autoplot(ar1, main = "AR1")


# Ahora, examinemos ACF:
# Calcula la funci√≥n de autocorrelaci√≥n (ACF) de la serie temporal ar1 sin generar un gr√°fico inmediatamente.
# Utiliza autoplot del paquete forecast para graficar la funci√≥n de autocorrelaci√≥n.
forecast::autoplot(acf(ar1, plot = F), main = "ACF of simulated AR1")



# El gr√°fico ACF muestra que las correlaciones disminuyen exponencialmente a medida que
# aumenta el Lag (retraso). Las l√≠neas azules punteadas indican las bandas de confianza de una
# correlaci√≥n significativa. Cualquier l√≠nea que se extienda por encima o por debajo de la banda
# m√≠nima se considera significativa. Adem√°s de ACF, tambi√©n deber√≠amos examinar la funci√≥n
# de autocorrelaci√≥n parcial (PACF, Partial Autocorrelation Function). La PACF es una
# correlaci√≥n condicional, lo que significa que la correlaci√≥n entre Yt e Yt-h est√° condicionada
# a las observaciones que se encuentran entre los dos. Una forma de entender esto
# intuitivamente es pensar en un modelo de regresi√≥n lineal y sus coeficientes. Supongamos
# que tienes Y = ÔÅ¢0 + ÔÅ¢1X1 versus Y = ÔÅ¢0 + ÔÅ¢1X1 + ÔÅ¢2X2. La relaci√≥n de X a Y en el primer
# modelo es lineal con un coeficiente, pero en el segundo modelo, el coeficiente ser√° diferente
# debido a que ahora tambi√©n se tiene en cuenta la relaci√≥n entre Y y X2. Ten en cuenta que, en
# el siguiente gr√°fico PACF, el valor de autocorrelaci√≥n parcial en el lag-1 es id√©ntico al valor
# de autocorrelaci√≥n en el lag-1, ya que no se trata de una correlaci√≥n condicional:

forecast::autoplot(pacf(ar1, plot = F), main = "PACF of simulated AR1")

# Podemos suponer con seguridad que la serie es estacionaria a partir de la apariencia del
# gr√°fico de la serie temporal anterior. Veremos un par de pruebas estad√≠sticas en el ejercicio
# pr√°ctico para asegurarnos de que los datos sean estacionarios pero, en ocasiones, la prueba
# del globo ocular (eyeball test) es suficiente. Si los datos no son estacionarios, entonces es
# posible eliminar la tendencia de los datos tomando sus diferencias. 

# Este es el Integrado (I) en ARIMA. Despu√©s de diferenciar, la nueva serie se convierte en
# ŒîYt = Yt - Yt-1. Se deber√≠a esperar que una diferencia de primer orden alcance la 
# estacionariedad pero, en algunas ocasiones, puede ser necesaria una diferencia de segundo
# orden. Un modelo ARIMA con AR(1) e I(1) se anotar√≠a como (1, 1, 0).
# MA significa media m√≥vil (Moving Average). Este no es el promedio m√≥vil simple como el
# promedio m√≥vil de 50 d√≠as del precio de una acci√≥n, sino m√°s bien un coeficiente que se
# aplica a los errores. Por supuesto, los errores se distribuyen de forma id√©ntica e independiente
# con una media cero y una varianza constante. La f√≥rmula para un modelo MA(1) es Yt =
# constante + Et + ŒòEt-1. Como hicimos con el modelo AR(1), podemos construir un MA(1)
# en R, de la siguiente manera:

set.seed(123)
ma1 <- arima.sim(list(order = c(0, 0, 1), ma = -0.5), n = 200)
forecast::autoplot(ma1, main = "MA1")

# Los gr√°ficos ACF y PACF son un poco diferentes del modelo AR(1). Ten en cuenta que
# existen algunas reglas generales al observar los gr√°ficos para determinar si el modelo tiene
# t√©rminos AR y/o MA. Pueden ser un poco subjetivos, as√≠ que dejaremos que tu aprendas estas
# heur√≠sticas, pero confiamos en R para identificar el modelo adecuado. En los siguientes
# gr√°ficos, veremos una correlaci√≥n significativa en el retraso 1 y dos correlaciones parciales
# significativas en el retraso 1 y el retraso 2:
forecast::autoplot(acf(ma1, plot = F), main = "ACF of simulated MA1")

# La figura anterior es el gr√°fico ACF y ahora veremos el gr√°fico PACF:
forecast::autoplot(pacf(ma1, plot = F), main = "PACF of simulated MA1")

# Con los modelos ARIMA es posible incorporar la estacionalidad, incluidos los t√©rminos
# autorregresivos, integrados y de media m√≥vil. La notaci√≥n del modelo ARIMA no estacional
# suele ser (p, d, q). Con ARIMA estacional, supongamos que los datos son mensuales,
# entonces la notaci√≥n ser√≠a (p, d, q) √ó (P, D, Q)12, y el 12 en la notaci√≥n tomar√≠a en cuenta la
# estacionalidad mensual. En los paquetes que usaremos, R puede identificar autom√°ticamente
# si se debe incluir la estacionalidad; de ser as√≠, tambi√©n se incluir√°n los t√©rminos √≥ptimos.



# Comprender la causalidad de Granger
# Imagina que te hacen una pregunta como: ¬øCu√°l es la relaci√≥n entre el n√∫mero de recetas
# nuevas y el total de recetas del medicamento X? Tu sabes que estos se miden mensualmente,
# entonces, ¬øqu√© podr√≠as hacer para comprender esa relaci√≥n, dado que la gente cree que los
# nuevos guiones aumentar√°n el total de guiones? ¬øO qu√© tal si se prueba la hip√≥tesis de que
# los precios de las materias primas (en particular, el cobre) son un indicador adelantado de los
# precios del mercado de valores en Estados Unidos?
#   Bueno, con dos conjuntos de datos de series temporales, x e y, la causalidad de Granger es
# un m√©todo que intenta determinar si es probable que una serie influya en un cambio en la
# otra. Esto se hace tomando diferentes rezagos de una serie y utiliz√°ndolos para modelar el
# cambio en la segunda serie. Para lograr esto, crearemos dos modelos que predecir√°n y, uno
# solo con los valores pasados de y(Œ©) y el otro con los valores pasados de y y x(œÄ). Los
# modelos son los siguientes, donde k es el n√∫mero de rezagos en la serie de tiempo:
#   Sea Œ© = ùë¶ùë° = ùõΩ0 + ùõΩ1ùë¶ùë° ‚àí 1 + ‚ãØ ùõΩùëòùë¶ùë° ‚àí ùëò + ùúñ y
# sea ùúã = ùë¶ùë° = ùõΩ0 + ùõΩ1ùë¶ùë° ‚àí 1 + ‚ãØ + ùõΩùëòùë¶ùë° ‚àí ùëò + ùõº1ùë¶ùë° ‚àí 1 + ‚ãØ + ùõºùëòùë¶ùë° ‚àí ùëò + ùúñ
# Luego se compara el RSS y se utiliza la prueba F para determinar si el modelo anidado (Œ©)
# es lo suficientemente adecuado para explicar los valores futuros de y o si el modelo completo
# (œÄ) es mejor. La prueba F se utiliza para probar las siguientes hip√≥tesis nulas y alternativas:
#   ‚Ä¢ H0:ÔÅ°1=0 para cada iÔÉé[1, k], sin causalidad de Granger
# ‚Ä¢ H1: ÔÅ°1‚â†0 para al menos un iÔÉé[1, k], causalidad de Granger

# B√°sicamente, estamos tratando de determinar si podemos decir que, estad√≠sticamente, x
# proporciona m√°s informaci√≥n sobre los valores futuros de y que los valores pasados de y
# solos. En esta definici√≥n, est√° claro que no estamos tratando de probar una causalidad real,
# s√≥lo que los dos valores est√°n relacionados por alg√∫n fen√≥meno. En este sentido, tambi√©n
# debemos ejecutar este modelo a la inversa para verificar que y no proporciona informaci√≥n
# sobre los valores futuros de x. Si encontramos que este es el caso, es probable que haya alguna
# variable ex√≥gena, digamos Z, que deba controlarse o que posiblemente sea una mejor
# candidata para la causalidad de Granger. Originalmente, hab√≠a que aplicar el m√©todo a series
# temporales estacionarias para evitar resultados espurios. Este ya no es el caso como se
# demostrar√°.

# Hay un par de formas diferentes de identificar la estructura de retraso (lag) adecuada.
# Naturalmente, podemos utilizar la fuerza bruta y la ignorancia para probar todos los retrasos
# razonables, uno a la vez. Es posible que tengamos una intuici√≥n racional basada en la
# experiencia en el dominio o quiz√°s en investigaciones previas que existan para guiar la
# selecci√≥n del retraso.
# De lo contrario, puedes aplicar auto regresi√≥n vectorial (VAR, Vector Autoregression) para
# identificar la estructura de retraso con el criterio de informaci√≥n m√°s bajo, como el criterio
# de informaci√≥n de Aikake (AIC, Aikake‚Äôs Information Criterion) o el error de predicci√≥n
# final (FPE, Final Prediction Error). Para simplificar, aqu√≠ est√° la notaci√≥n para los modelos
# VAR con dos variables, y esto incorpora solo un lag para cada variable. Esta notaci√≥n se
# puede ampliar para tantas variables y lags como sea apropiado:
#   ‚Ä¢ Y = constante1 + B11Yt-1 + B12Yt-1 + e1
# ‚Ä¢ X = constante1 + B21Yt-1 + B22Yt-1 + e2
# En R, este proceso es bastante sencillo de implementar como veremos en el siguiente
# problema pr√°ctico.



# ------------------------------------------------------------------------------

# Datos de series de tiempo

# El cambio clim√°tico est√° ocurriendo. Siempre lo ha sido y lo ser√°, pero la gran pregunta, al
# menos desde un punto de vista pol√≠tico y econ√≥mico, ¬øes el cambio clim√°tico provocado por
# el hombre? Utilizaremos esta parte del documento para poner a prueba el modelado
# econom√©trico de series temporales para tratar de aprender si las emisiones de carbono causan,
# estad√≠sticamente hablando, el cambio clim√°tico y, en particular, el aumento de las
# temperaturas.

# Los datos que usaremos se proporcionan como una anomal√≠a anual, que se calcula como la
# diferencia de la temperatura superficial anual media para un per√≠odo de tiempo determinado
# versus el promedio de los a√±os de referencia (1961-1990). La temperatura superficial anual
# es un conjunto de temperaturas recopiladas a nivel mundial y combinadas a partir de los
# conjuntos de datos de temperatura del aire en la superficie CRUTEM4 y de la superficie del
# mar HadSST3. Los esc√©pticos han atacado a los parciales y poco fiables:
#   https://www.telegraph.co.uk/comment/11561629/Top-scientists-start-to-examine-fiddledglobal-warming-figures.html. Esto est√° muy fuera de nuestro alcance de esfuerzo aqu√≠, por
# lo que debemos aceptar y utilizar estos datos tal como est√°n, pero de todos modos lo
# encontramos divertido. Obtuvimos los datos desde 1919 hasta 2013 para que coincidan con
# nuestros datos de CO2.

# Las estimaciones de emisiones globales de CO2 se pueden encontrar en el Centro de An√°lisis
# de Informaci√≥n sobre Di√≥xido de Carbono (CDIAC) del Departamento de Energ√≠a de EE.
# UU. en el siguiente sitio web: https://data.globalchange.gov/organization/carbon-dioxideinformation-analysis-center.
# Instalemos bibliotecas seg√∫n sea necesario, carguemos los datos y examinemos la estructura:
library(magrittr)
library(ggthemes)
library(tseries)
library(ggplot2)
library(tidyverse)
climate <- readr::read_csv("climate.csv")
str(climate)


# Pondremos esto en una estructura de serie temporal, especificando los a√±os de inicio y
# finalizaci√≥n:
climate_ts <- ts(climate[, 2:3],
                 start = 1919,
                 end = 2013)
#   Con nuestros datos cargados y colocados en estructuras de series temporales, ahora podemos
# comenzar a comprenderlos y prepararlos a√∫n m√°s para el an√°lisis.


# Exploraci√≥n de datos
# Comencemos con una gr√°fica de la serie temporal usando la base de R:

plot(climate_ts, main = "CO2 and Temperature Deviation")

# Parece que los niveles de CO2 realmente comenzaron a aumentar despu√©s de la Segunda
# Guerra Mundial y hay un r√°pido aumento de las anomal√≠as de temperatura a mediados de los
# a√±os 1970. No parece haber valores at√≠picos obvios y la variaci√≥n a lo largo del tiempo parece
# constante.
# Utilizando el procedimiento est√°ndar, podemos ver que las dos series est√°n altamente
# correlacionadas, como sigue:
cor(climate_ts)


# Como se mencion√≥ anteriormente, esto no es motivo de alegr√≠a, ya que no prueba
# absolutamente nada. Buscaremos la estructura trazando ACF y PACF para ambas series:
forecast::autoplot(acf(climate_ts[, 2], plot = F), main="Temperature ACF")

# Este c√≥digo nos da el gr√°fico PACF para la temperatura:
forecast::autoplot(pacf(climate_ts[, 2], plot = F), main = "Temperature PACF")

# Este c√≥digo nos da el gr√°fico ACF para CO2:
forecast::autoplot(acf(climate_ts[, 1], plot = F), main = "CO2 ACF")

# Este c√≥digo nos da el gr√°fico PACF para CO2:
forecast::autoplot(acf(climate_ts[, 1], plot = F), main = "CO2 PACF")

# Con los patrones ACF que decaen lentamente y los patrones PACF que decaen r√°pidamente,
# podemos suponer que ambas series son autorregresivas, aunque Temp parece tener algunos
# t√©rminos MA significativos. A continuaci√≥n, echemos un vistazo a la funci√≥n de correlaci√≥n
# cruzada (CCF, Cross-Correlation Function). Ten en cuenta que ponemos nuestra x antes de
# nuestra y en la funci√≥n:

forecast::autoplot(ccf(climate_ts[, 1], climate_ts[, 2], plot = F), main = "CCF")

# El CCF nos muestra la correlaci√≥n entre la temperatura y los lags de CO2. Si los lags
# negativos de la variable x tienen una correlaci√≥n alta, podemos decir que x adelanta a y. Si
# los lags positivos de x tienen una correlaci√≥n alta, decimos que x lags y. Aqu√≠ podemos ver
# que el CO2 es una variable tanto adelantada como retrasada. Para nuestro an√°lisis, es
# alentador que veamos lo primero, pero extra√±o que veamos lo segundo. Veremos durante el
# an√°lisis de causalidad del VAR y de Granger si esto importar√° o no.
# Adem√°s, necesitamos probar si los datos son estacionarios. Podemos probar esto con la
# prueba Augmented Dickey-Fuller (ADF) disponible en el paquete tseries, usando la funci√≥n
# adf.test(), de la siguiente manera:

tseries::adf.test(climate_ts[, 1])
tseries::adf.test(climate_ts[, 2])

# Para ambas series, tenemos valores p insignificantes, por lo que no podemos rechazar el valor
# nulo y concluir que no son estacionarios.
# Habiendo explorado los datos, comencemos el proceso de modelado, comenzando con la
# aplicaci√≥n de t√©cnicas univariadas a las anomal√≠as de temperatura.


# Modelado y evaluaci√≥n
# Para el paso de modelado y evaluaci√≥n, nos centraremos en tres tareas. La primera es producir
# un modelo de pron√≥stico univariado aplicado √∫nicamente a la temperatura de la superficie.
# El segundo es desarrollar un modelo de auto regresi√≥n vectorial de la temperatura de la
# superficie y los niveles de CO2, utilizando ese resultado para informar nuestro trabajo sobre
# si los niveles de CO2 causan las anomal√≠as de la temperatura de la superficie.
# Previsi√≥n de series temporales univariadas
# Con esta tarea, el objetivo es producir un pron√≥stico univariado para la temperatura de la
# superficie, centr√°ndose en elegir un modelo de suavizado exponencial, un modelo ARIMA o
# un conjunto de m√©todos, incluida una red neuronal. Entrenaremos los modelos y
# determinaremos su precisi√≥n predictiva en un conjunto de pruebas fuera de tiempo, tal como
# lo hemos hecho en otros esfuerzos de aprendizaje. El siguiente c√≥digo crea el entrenamiento
# y los conjuntos de prueba:
  
temp_ts <- ts(climate$Temp, start = 1919, frequency = 1)
train <- window(temp_ts, end = 2007)
test <- window(temp_ts, start = 2008)

# Para construir nuestro modelo de suavizado exponencial, usaremos la funci√≥n ets() que se
# encuentra en el paquete forecast. La funci√≥n encontrar√° el mejor modelo con el AIC m√°s
# bajo:
  
fit.ets <- forecast::ets(train)
fit.ets

# El objeto modelo devuelve una serie de par√°metros de inter√©s. Lo primero que hay que
# comprobar es qu√© significa (A, A, N). Representa que el modelo seleccionado es un
# suavizado exponencial simple con errores aditivos. La primera letra indica el tipo de error, la 
# segunda letra la tendencia y la tercera letra la estacionalidad. Las letras posibles son las
# siguientes:
#   ‚Ä¢ A = aditivo
# ‚Ä¢ M = multiplicativo
# ‚Ä¢ N = ninguno
# Tambi√©n vemos las estimaciones de par√°metros con alfa, el par√°metro de suavizado, para la
# correcci√≥n de errores (el nivel) y beta para la pendiente. Los valores del estado inicial se
# utilizaron para iniciar la selecci√≥n del modelo; sigma es la variaci√≥n de los residuos y se
# proporcionan los valores de los criterios del modelo. Puedes trazar c√≥mo cambian las
# estimaciones con el tiempo:
forecast::autoplot(fit.ets)


# Ahora trazaremos el pron√≥stico y veremos qu√© tan bien se desempe√±√≥ visualmente en los
# datos de prueba:
plot(forecast::forecast(fit.ets, h = 6))
lines(test, type = "o")


# Mirando el gr√°fico, parece que este pron√≥stico muestra una ligera tendencia alcista lineal y
# est√° sobreestimando los valores reales. Ahora veremos las medidas de precisi√≥n del modelo:
fit.ets %>% forecast::forecast(h = 6) %>%  forecast::accuracy(temp_ts)


# Hay ocho medidas de error. En el que creo que deber√≠amos centrarnos es en la U de Theil (en
# realidad, la U2, ya que la U de Theil original ten√≠a algunos defectos), que s√≥lo est√° disponible
# en los datos de prueba. La U de Theil es una estad√≠stica interesante ya que no depende de la
# escala, por lo que se pueden comparar varios modelos.
# Por ejemplo, si en un modelo transformas la serie temporal usando una escala logar√≠tmica,
# puedes comparar la estad√≠stica con un modelo que no transforma los datos. Puedes
# considerarlo como la proporci√≥n en la que el pron√≥stico mejora la previsibilidad con respecto
# a un pron√≥stico ingenuo, o podemos describirlo como la ra√≠z del error cuadr√°tico medio
# (RMSE) del modelo dividido por el RMSE de un modelo ingenuo.
# Por lo tanto, las estad√≠sticas U de Theil mayores que 1 funcionan peor que un pron√≥stico
# ingenuo, un valor de 1 equivale a ingenuo y menos de 1 indica que el modelo tiene un
# rendimiento ingenuo. M√°s informaci√≥n sobre c√≥mo se deriva la estad√≠stica est√° disponible en
# este enlace:
#   https://www.researchgate.net/publication/323754973_Forecasting_methods_and_principles
# _Evidence-based_checklists_forecastingprinciplescom_forprincom
# El modelo de suavizado proporcion√≥ una estad√≠stica de 0.7940449. Eso no es muy
# impresionante a pesar de que est√° por debajo de uno. En la opini√≥n de los expertos,
# deber√≠amos esforzarnos por alcanzar valores iguales o inferiores a 0.5.
# Ahora desarrollaremos un modelo ARIMA, usando auto.arima(), que tambi√©n pertenece al
# paquete forecast.
# Hay muchas opciones que puedes especificar en la funci√≥n, o simplemente puedes incluir los
# datos de tu serie de tiempo y encontrar√°s el mejor ajuste ARIMA. Recomendamos utilizar la
# funci√≥n con precauci√≥n, ya que a menudo puede devolver un modelo que viola los supuestos
# para los residuos, como veremos:
fit.arima <- forecast::auto.arima(train)
summary(fit.arima)


# El resultado abreviado muestra que el modelo seleccionado es AR = 0, I = 1 y MA = 4, I = 1, o
# ARIMA(0,1,4). Podemos examinar el gr√°fico de su desempe√±o en los datos de prueba de la misma
# manera que antes:
plot(forecast::forecast(fit.arima, h = 6))
lines(test, type = "o")