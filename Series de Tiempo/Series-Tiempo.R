# Series de tiempo y causalidad

# Una serie de tiempo univariada es donde las mediciones se recopilan durante una medida de
# tiempo est√°ndar, que podr√≠a ser por minuto, hora, d√≠a, semana, mes, etc. Lo que hace que las
# series temporales sean m√°s problem√°ticas que otros datos es que el orden de las
# observaciones importa. Esta dependencia del orden puede hacer que los m√©todos de an√°lisis
# est√°ndar produzcan un sesgo o una variaci√≥n innecesariamente altos.

# Otro aspecto de las series temporales que a menudo se pasa por alto es la causalidad. S√≠, no
# queremos confundir correlaci√≥n con causalidad, pero, en el an√°lisis de series temporales,
# podemos aplicar la t√©cnica de causalidad de Granger para determinar si la causalidad,
# estad√≠sticamente hablando, existe.
# Aplicaremos series de tiempo/t√©cnicas econom√©tricas para identificar
# modelos de pron√≥stico univariados (incluidos conjuntos), modelos de vectores
# autorregresivos y, finalmente, la causalidad de Granger. 

# es posible que no domines completamente el an√°lisis de series de tiempo, pero
# sabr√°s lo suficiente para realizar un an√°lisis efectivo y comprender las cuestiones
# fundamentales a considerar al construir modelos de series de tiempo y crear modelos
# predictivos (pron√≥sticos).
# Los siguientes son los temas que se cubrir√°n:
# ‚Ä¢ An√°lisis univariado de series de tiempo
# ‚Ä¢ Datos de series de tiempo
# ‚Ä¢ Modelado y evaluaci√≥n




# An√°lisis univariado de series temporales

# Nos centraremos en dos m√©todos para analizar y pronosticar una √∫nica serie temporal:
# modelos de suavizado exponencial (exponential smoothing) y media m√≥vil integrada
# autorregresiva (ARIMA, Autoregressive Integrated Moving Average). Comenzaremos
# analizando modelos de suavizado exponencial.
# Al igual que los modelos de media m√≥vil, los modelos de suavizado exponencial utilizan
# ponderaciones para observaciones pasadas. Pero a diferencia de los modelos de media m√≥vil, 
# cuanto m√°s reciente es la observaci√≥n, m√°s peso se le da en relaci√≥n con las posteriores. Hay
# tres posibles par√°metros de suavizado para estimar: el par√°metro de suavizado general, un
# par√°metro de tendencia y el par√°metro de suavizado estacional. Si no hay tendencia o
# estacionalidad, estos par√°metros se vuelven nulos.

# El par√°metro de suavizado produce un pron√≥stico (forecast) con la siguiente ecuaci√≥n:
#   ùëåùë° + 1 = ùõº(ùëåùë°) + (1 ‚àí ùõº)ùëåùë° ‚àí 1 + (1 ‚àí ùõº)2ùëåùë° + ‚ãØ , donde 0 < ùõº ‚â§ 1

# En esta ecuaci√≥n, Yt es el valor en ese momento, T, y alfa (Œ±) es el par√°metro de suavizado.
# Los algoritmos optimizan el alfa (y otros par√°metros) minimizando los errores, la suma de
# errores cuadr√°ticos (SSE, Sum of Squared Error) o la m√°xima verosimilitud.
# La ecuaci√≥n de pron√≥stico junto con las ecuaciones de tendencia y estacionalidad, si
# corresponde, ser√° la siguiente:
#   ‚Ä¢ El pron√≥stico, donde A es la ecuaci√≥n de suavizado anterior y h es el n√∫mero de
# per√≠odos de pron√≥stico: ùëåùë° + ‚Ñé = ùê¥ + ‚Ñéùêµùë° + ùëÜùë°
# ‚Ä¢ La ecuaci√≥n de tendencia: ùêµùë° = ùõΩ(ùê¥ùë° ‚àí ùê¥ùë° ‚àí 1) + (1 ‚àí ùõΩ)ùêµùë° ‚àí 1
# ‚Ä¢ La estacionalidad, donde m es el n√∫mero de per√≠odos estacionales:
#   ùëÜùë° = Œ©(ùëåùë° ‚àí ùê¥ùë° ‚àí 1 ‚àí ùêµùë° ‚àí 1) + (1 ‚àí Œ©)ùëÜùë° ‚àí ùëö

# Esta ecuaci√≥n se conoce como m√©todo de Holt-Winters. La ecuaci√≥n de pron√≥stico es de
# naturaleza aditiva y la tendencia es lineal. El m√©todo tambi√©n permite la inclusi√≥n de una
# tendencia amortiguada y una estacionalidad multiplicativa, donde la estacionalidad aumenta
# o disminuye proporcionalmente con el tiempo. Con estos modelos, no tienes que preocuparte
# por el supuesto de estacionariedad como en un modelo ARIMA. La estacionariedad es
# cuando la serie de tiempo tiene una media, varianza y correlaci√≥n constantes entre todos los
# per√≠odos de tiempo. Dicho esto, sigue siendo importante comprender los modelos ARIMA,
# ya que habr√° situaciones en las que tendr√°n el mejor rendimiento.
# Comenzando con el modelo autorregresivo, el valor de Y en el tiempo T es una funci√≥n lineal
# de los valores anteriores de Y. La f√≥rmula para un modelo autorregresivo de retraso lag-11
# AR(1) es ùëåùë° = constante + Œ¶ùëåùë° ‚àí 1 + ùê∏ùë°. Los supuestos cr√≠ticos para el modelo son los
# siguientes:
#   ‚Ä¢ Et denota los errores que se distribuyen de manera id√©ntica e independiente con una
# media cero y una varianza constante
# ‚Ä¢ Los errores son independientes de Yt
# ‚Ä¢ Yt, Yt-1, Yt-n... es estacionario, lo que significa que el valor absoluto de Œ¶ es menor
# que uno

# Con una serie temporal estacionaria, se puede examinar la funci√≥n de autocorrelaci√≥n (ACF, Autocorrelation Function).
# El ACF de una serie estacionaria proporciona correlaciones entre
# Yt e Yt-h para h = 1, 2...n. Usemos R para crear una serie AR(1) y trazarla:

install.packages("forecast")
library(forecast)
set.seed(1966)

# este comando simula una serie temporal de un modelo autorregresivo de orden 1 (AR(1)).
# especifica el modelo AR(1) con un coeficiente autorregresivo de 0.5.
# order = c(1, 0, 0) indica que el modelo tiene un t√©rmino autorregresivo de primer orden (1), sin diferencias (0), y sin t√©rminos de media m√≥vil (0).
# ar = 0.5 especifica el coeficiente del t√©rmino autorregresivo.
# n = 200 indica que se deben generar 200 observaciones para la serie temporal.
ar1 <- arima.sim(list(order = c(1, 0, 0), ar = 0.5), n = 200)
# Este comando utiliza la funci√≥n autoplot del paquete forecast para graficar la serie temporal simulada.
forecast::autoplot(ar1, main = "AR1")


# Ahora, examinemos ACF:
# Calcula la funci√≥n de autocorrelaci√≥n (ACF) de la serie temporal ar1 sin generar un gr√°fico inmediatamente.
# Utiliza autoplot del paquete forecast para graficar la funci√≥n de autocorrelaci√≥n.
forecast::autoplot(acf(ar1, plot = F), main = "ACF of simulated AR1")



# El gr√°fico ACF muestra que las correlaciones disminuyen exponencialmente a medida que
# aumenta el Lag (retraso). Las l√≠neas azules punteadas indican las bandas de confianza de una
# correlaci√≥n significativa. Cualquier l√≠nea que se extienda por encima o por debajo de la banda
# m√≠nima se considera significativa. Adem√°s de ACF, tambi√©n deber√≠amos examinar la funci√≥n
# de autocorrelaci√≥n parcial (PACF, Partial Autocorrelation Function). La PACF es una
# correlaci√≥n condicional, lo que significa que la correlaci√≥n entre Yt e Yt-h est√° condicionada
# a las observaciones que se encuentran entre los dos. Una forma de entender esto
# intuitivamente es pensar en un modelo de regresi√≥n lineal y sus coeficientes. Supongamos
# que tienes Y = ÔÅ¢0 + ÔÅ¢1X1 versus Y = ÔÅ¢0 + ÔÅ¢1X1 + ÔÅ¢2X2. La relaci√≥n de X a Y en el primer
# modelo es lineal con un coeficiente, pero en el segundo modelo, el coeficiente ser√° diferente
# debido a que ahora tambi√©n se tiene en cuenta la relaci√≥n entre Y y X2. Ten en cuenta que, en
# el siguiente gr√°fico PACF, el valor de autocorrelaci√≥n parcial en el lag-1 es id√©ntico al valor
# de autocorrelaci√≥n en el lag-1, ya que no se trata de una correlaci√≥n condicional:

forecast::autoplot(pacf(ar1, plot = F), main = "PACF of simulated AR1")

# Podemos suponer con seguridad que la serie es estacionaria a partir de la apariencia del
# gr√°fico de la serie temporal anterior. Veremos un par de pruebas estad√≠sticas en el ejercicio
# pr√°ctico para asegurarnos de que los datos sean estacionarios pero, en ocasiones, la prueba
# del globo ocular (eyeball test) es suficiente. Si los datos no son estacionarios, entonces es
# posible eliminar la tendencia de los datos tomando sus diferencias. 

# Este es el Integrado (I) en ARIMA. Despu√©s de diferenciar, la nueva serie se convierte en
# ŒîYt = Yt - Yt-1. Se deber√≠a esperar que una diferencia de primer orden alcance la 
# estacionariedad pero, en algunas ocasiones, puede ser necesaria una diferencia de segundo
# orden. Un modelo ARIMA con AR(1) e I(1) se anotar√≠a como (1, 1, 0).
# MA significa media m√≥vil (Moving Average). Este no es el promedio m√≥vil simple como el
# promedio m√≥vil de 50 d√≠as del precio de una acci√≥n, sino m√°s bien un coeficiente que se
# aplica a los errores. Por supuesto, los errores se distribuyen de forma id√©ntica e independiente
# con una media cero y una varianza constante. La f√≥rmula para un modelo MA(1) es Yt =
# constante + Et + ŒòEt-1. Como hicimos con el modelo AR(1), podemos construir un MA(1)
# en R, de la siguiente manera:

set.seed(123)
ma1 <- arima.sim(list(order = c(0, 0, 1), ma = -0.5), n = 200)
forecast::autoplot(ma1, main = "MA1")

# Los gr√°ficos ACF y PACF son un poco diferentes del modelo AR(1). Ten en cuenta que
# existen algunas reglas generales al observar los gr√°ficos para determinar si el modelo tiene
# t√©rminos AR y/o MA. Pueden ser un poco subjetivos, as√≠ que dejaremos que tu aprendas estas
# heur√≠sticas, pero confiamos en R para identificar el modelo adecuado. En los siguientes
# gr√°ficos, veremos una correlaci√≥n significativa en el retraso 1 y dos correlaciones parciales
# significativas en el retraso 1 y el retraso 2:
forecast::autoplot(acf(ma1, plot = F), main = "ACF of simulated MA1")

# La figura anterior es el gr√°fico ACF y ahora veremos el gr√°fico PACF:
forecast::autoplot(pacf(ma1, plot = F), main = "PACF of simulated MA1")

# Con los modelos ARIMA es posible incorporar la estacionalidad, incluidos los t√©rminos
# autorregresivos, integrados y de media m√≥vil. La notaci√≥n del modelo ARIMA no estacional
# suele ser (p, d, q). Con ARIMA estacional, supongamos que los datos son mensuales,
# entonces la notaci√≥n ser√≠a (p, d, q) √ó (P, D, Q)12, y el 12 en la notaci√≥n tomar√≠a en cuenta la
# estacionalidad mensual. En los paquetes que usaremos, R puede identificar autom√°ticamente
# si se debe incluir la estacionalidad; de ser as√≠, tambi√©n se incluir√°n los t√©rminos √≥ptimos.



# Comprender la causalidad de Granger
# Imagina que te hacen una pregunta como: ¬øCu√°l es la relaci√≥n entre el n√∫mero de recetas
# nuevas y el total de recetas del medicamento X? Tu sabes que estos se miden mensualmente,
# entonces, ¬øqu√© podr√≠as hacer para comprender esa relaci√≥n, dado que la gente cree que los
# nuevos guiones aumentar√°n el total de guiones? ¬øO qu√© tal si se prueba la hip√≥tesis de que
# los precios de las materias primas (en particular, el cobre) son un indicador adelantado de los
# precios del mercado de valores en Estados Unidos?
#   Bueno, con dos conjuntos de datos de series temporales, x e y, la causalidad de Granger es
# un m√©todo que intenta determinar si es probable que una serie influya en un cambio en la
# otra. Esto se hace tomando diferentes rezagos de una serie y utiliz√°ndolos para modelar el
# cambio en la segunda serie. Para lograr esto, crearemos dos modelos que predecir√°n y, uno
# solo con los valores pasados de y(Œ©) y el otro con los valores pasados de y y x(œÄ). Los
# modelos son los siguientes, donde k es el n√∫mero de rezagos en la serie de tiempo:
#   Sea Œ© = ùë¶ùë° = ùõΩ0 + ùõΩ1ùë¶ùë° ‚àí 1 + ‚ãØ ùõΩùëòùë¶ùë° ‚àí ùëò + ùúñ y
# sea ùúã = ùë¶ùë° = ùõΩ0 + ùõΩ1ùë¶ùë° ‚àí 1 + ‚ãØ + ùõΩùëòùë¶ùë° ‚àí ùëò + ùõº1ùë¶ùë° ‚àí 1 + ‚ãØ + ùõºùëòùë¶ùë° ‚àí ùëò + ùúñ
# Luego se compara el RSS y se utiliza la prueba F para determinar si el modelo anidado (Œ©)
# es lo suficientemente adecuado para explicar los valores futuros de y o si el modelo completo
# (œÄ) es mejor. La prueba F se utiliza para probar las siguientes hip√≥tesis nulas y alternativas:
#   ‚Ä¢ H0:ÔÅ°1=0 para cada iÔÉé[1, k], sin causalidad de Granger
# ‚Ä¢ H1: ÔÅ°1‚â†0 para al menos un iÔÉé[1, k], causalidad de Granger

# B√°sicamente, estamos tratando de determinar si podemos decir que, estad√≠sticamente, x
# proporciona m√°s informaci√≥n sobre los valores futuros de y que los valores pasados de y
# solos. En esta definici√≥n, est√° claro que no estamos tratando de probar una causalidad real,
# s√≥lo que los dos valores est√°n relacionados por alg√∫n fen√≥meno. En este sentido, tambi√©n
# debemos ejecutar este modelo a la inversa para verificar que y no proporciona informaci√≥n
# sobre los valores futuros de x. Si encontramos que este es el caso, es probable que haya alguna
# variable ex√≥gena, digamos Z, que deba controlarse o que posiblemente sea una mejor
# candidata para la causalidad de Granger. Originalmente, hab√≠a que aplicar el m√©todo a series
# temporales estacionarias para evitar resultados espurios. Este ya no es el caso como se
# demostrar√°.

# Hay un par de formas diferentes de identificar la estructura de retraso (lag) adecuada.
# Naturalmente, podemos utilizar la fuerza bruta y la ignorancia para probar todos los retrasos
# razonables, uno a la vez. Es posible que tengamos una intuici√≥n racional basada en la
# experiencia en el dominio o quiz√°s en investigaciones previas que existan para guiar la
# selecci√≥n del retraso.
# De lo contrario, puedes aplicar auto regresi√≥n vectorial (VAR, Vector Autoregression) para
# identificar la estructura de retraso con el criterio de informaci√≥n m√°s bajo, como el criterio
# de informaci√≥n de Aikake (AIC, Aikake‚Äôs Information Criterion) o el error de predicci√≥n
# final (FPE, Final Prediction Error). Para simplificar, aqu√≠ est√° la notaci√≥n para los modelos
# VAR con dos variables, y esto incorpora solo un lag para cada variable. Esta notaci√≥n se
# puede ampliar para tantas variables y lags como sea apropiado:
#   ‚Ä¢ Y = constante1 + B11Yt-1 + B12Yt-1 + e1
# ‚Ä¢ X = constante1 + B21Yt-1 + B22Yt-1 + e2
# En R, este proceso es bastante sencillo de implementar como veremos en el siguiente
# problema pr√°ctico.



# ------------------------------------------------------------------------------

# Datos de series de tiempo

# El cambio clim√°tico est√° ocurriendo. Siempre lo ha sido y lo ser√°, pero la gran pregunta, al
# menos desde un punto de vista pol√≠tico y econ√≥mico, ¬øes el cambio clim√°tico provocado por
# el hombre? Utilizaremos esta parte del documento para poner a prueba el modelado
# econom√©trico de series temporales para tratar de aprender si las emisiones de carbono causan,
# estad√≠sticamente hablando, el cambio clim√°tico y, en particular, el aumento de las
# temperaturas.

# Los datos que usaremos se proporcionan como una anomal√≠a anual, que se calcula como la
# diferencia de la temperatura superficial anual media para un per√≠odo de tiempo determinado
# versus el promedio de los a√±os de referencia (1961-1990). La temperatura superficial anual
# es un conjunto de temperaturas recopiladas a nivel mundial y combinadas a partir de los
# conjuntos de datos de temperatura del aire en la superficie CRUTEM4 y de la superficie del
# mar HadSST3. Los esc√©pticos han atacado a los parciales y poco fiables:
#   https://www.telegraph.co.uk/comment/11561629/Top-scientists-start-to-examine-fiddledglobal-warming-figures.html. Esto est√° muy fuera de nuestro alcance de esfuerzo aqu√≠, por
# lo que debemos aceptar y utilizar estos datos tal como est√°n, pero de todos modos lo
# encontramos divertido. Obtuvimos los datos desde 1919 hasta 2013 para que coincidan con
# nuestros datos de CO2.

# Las estimaciones de emisiones globales de CO2 se pueden encontrar en el Centro de An√°lisis
# de Informaci√≥n sobre Di√≥xido de Carbono (CDIAC) del Departamento de Energ√≠a de EE.
# UU. en el siguiente sitio web: https://data.globalchange.gov/organization/carbon-dioxideinformation-analysis-center.
# Instalemos bibliotecas seg√∫n sea necesario, carguemos los datos y examinemos la estructura:
library(magrittr)
library(ggthemes)
library(tseries)
library(ggplot2)
library(tidyverse)
climate <- readr::read_csv("climate.csv")
str(climate)


# Pondremos esto en una estructura de serie temporal, especificando los a√±os de inicio y
# finalizaci√≥n:
climate_ts <- ts(climate[, 2:3],
                 start = 1919,
                 end = 2013)
#   Con nuestros datos cargados y colocados en estructuras de series temporales, ahora podemos
# comenzar a comprenderlos y prepararlos a√∫n m√°s para el an√°lisis.


# Exploraci√≥n de datos
# Comencemos con una gr√°fica de la serie temporal usando la base de R:

plot(climate_ts, main = "CO2 and Temperature Deviation")

# Parece que los niveles de CO2 realmente comenzaron a aumentar despu√©s de la Segunda
# Guerra Mundial y hay un r√°pido aumento de las anomal√≠as de temperatura a mediados de los
# a√±os 1970. No parece haber valores at√≠picos obvios y la variaci√≥n a lo largo del tiempo parece
# constante.
# Utilizando el procedimiento est√°ndar, podemos ver que las dos series est√°n altamente
# correlacionadas, como sigue:
cor(climate_ts)


# Como se mencion√≥ anteriormente, esto no es motivo de alegr√≠a, ya que no prueba
# absolutamente nada. Buscaremos la estructura trazando ACF y PACF para ambas series:
forecast::autoplot(acf(climate_ts[, 2], plot = F), main="Temperature ACF")

# Este c√≥digo nos da el gr√°fico PACF para la temperatura:
forecast::autoplot(pacf(climate_ts[, 2], plot = F), main = "Temperature PACF")

# Este c√≥digo nos da el gr√°fico ACF para CO2:
forecast::autoplot(acf(climate_ts[, 1], plot = F), main = "CO2 ACF")

# Este c√≥digo nos da el gr√°fico PACF para CO2:
forecast::autoplot(acf(climate_ts[, 1], plot = F), main = "CO2 PACF")

# Con los patrones ACF que decaen lentamente y los patrones PACF que decaen r√°pidamente,
# podemos suponer que ambas series son autorregresivas, aunque Temp parece tener algunos
# t√©rminos MA significativos. A continuaci√≥n, echemos un vistazo a la funci√≥n de correlaci√≥n
# cruzada (CCF, Cross-Correlation Function). Ten en cuenta que ponemos nuestra x antes de
# nuestra y en la funci√≥n:

forecast::autoplot(ccf(climate_ts[, 1], climate_ts[, 2], plot = F), main = "CCF")

# El CCF nos muestra la correlaci√≥n entre la temperatura y los lags de CO2. Si los lags
# negativos de la variable x tienen una correlaci√≥n alta, podemos decir que x adelanta a y. Si
# los lags positivos de x tienen una correlaci√≥n alta, decimos que x lags y. Aqu√≠ podemos ver
# que el CO2 es una variable tanto adelantada como retrasada. Para nuestro an√°lisis, es
# alentador que veamos lo primero, pero extra√±o que veamos lo segundo. Veremos durante el
# an√°lisis de causalidad del VAR y de Granger si esto importar√° o no.
# Adem√°s, necesitamos probar si los datos son estacionarios. Podemos probar esto con la
# prueba Augmented Dickey-Fuller (ADF) disponible en el paquete tseries, usando la funci√≥n
# adf.test(), de la siguiente manera:

tseries::adf.test(climate_ts[, 1])
tseries::adf.test(climate_ts[, 2])

# Para ambas series, tenemos valores p insignificantes, por lo que no podemos rechazar el valor
# nulo y concluir que no son estacionarios.
# Habiendo explorado los datos, comencemos el proceso de modelado, comenzando con la
# aplicaci√≥n de t√©cnicas univariadas a las anomal√≠as de temperatura.


# Modelado y evaluaci√≥n
# Para el paso de modelado y evaluaci√≥n, nos centraremos en tres tareas. La primera es producir
# un modelo de pron√≥stico univariado aplicado √∫nicamente a la temperatura de la superficie.
# El segundo es desarrollar un modelo de auto regresi√≥n vectorial de la temperatura de la
# superficie y los niveles de CO2, utilizando ese resultado para informar nuestro trabajo sobre
# si los niveles de CO2 causan las anomal√≠as de la temperatura de la superficie.
# Previsi√≥n de series temporales univariadas
# Con esta tarea, el objetivo es producir un pron√≥stico univariado para la temperatura de la
# superficie, centr√°ndose en elegir un modelo de suavizado exponencial, un modelo ARIMA o
# un conjunto de m√©todos, incluida una red neuronal. Entrenaremos los modelos y
# determinaremos su precisi√≥n predictiva en un conjunto de pruebas fuera de tiempo, tal como
# lo hemos hecho en otros esfuerzos de aprendizaje. El siguiente c√≥digo crea el entrenamiento
# y los conjuntos de prueba:
  
temp_ts <- ts(climate$Temp, start = 1919, frequency = 1)
train <- window(temp_ts, end = 2007)
test <- window(temp_ts, start = 2008)

# Para construir nuestro modelo de suavizado exponencial, usaremos la funci√≥n ets() que se
# encuentra en el paquete forecast. La funci√≥n encontrar√° el mejor modelo con el AIC m√°s
# bajo:
  
fit.ets <- forecast::ets(train)
fit.ets

# El objeto modelo devuelve una serie de par√°metros de inter√©s. Lo primero que hay que
# comprobar es qu√© significa (A, A, N). Representa que el modelo seleccionado es un
# suavizado exponencial simple con errores aditivos. La primera letra indica el tipo de error, la 
# segunda letra la tendencia y la tercera letra la estacionalidad. Las letras posibles son las
# siguientes:
#   ‚Ä¢ A = aditivo
# ‚Ä¢ M = multiplicativo
# ‚Ä¢ N = ninguno
# Tambi√©n vemos las estimaciones de par√°metros con alfa, el par√°metro de suavizado, para la
# correcci√≥n de errores (el nivel) y beta para la pendiente. Los valores del estado inicial se
# utilizaron para iniciar la selecci√≥n del modelo; sigma es la variaci√≥n de los residuos y se
# proporcionan los valores de los criterios del modelo. Puedes trazar c√≥mo cambian las
# estimaciones con el tiempo:
forecast::autoplot(fit.ets)


# Ahora trazaremos el pron√≥stico y veremos qu√© tan bien se desempe√±√≥ visualmente en los
# datos de prueba:
plot(forecast::forecast(fit.ets, h = 6))
lines(test, type = "o")


# Mirando el gr√°fico, parece que este pron√≥stico muestra una ligera tendencia alcista lineal y
# est√° sobreestimando los valores reales. Ahora veremos las medidas de precisi√≥n del modelo:
fit.ets %>% forecast::forecast(h = 6) %>%  forecast::accuracy(temp_ts)


# Hay ocho medidas de error. En el que creo que deber√≠amos centrarnos es en la U de Theil (en
# realidad, la U2, ya que la U de Theil original ten√≠a algunos defectos), que s√≥lo est√° disponible
# en los datos de prueba. La U de Theil es una estad√≠stica interesante ya que no depende de la
# escala, por lo que se pueden comparar varios modelos.
# Por ejemplo, si en un modelo transformas la serie temporal usando una escala logar√≠tmica,
# puedes comparar la estad√≠stica con un modelo que no transforma los datos. Puedes
# considerarlo como la proporci√≥n en la que el pron√≥stico mejora la previsibilidad con respecto
# a un pron√≥stico ingenuo, o podemos describirlo como la ra√≠z del error cuadr√°tico medio
# (RMSE) del modelo dividido por el RMSE de un modelo ingenuo.
# Por lo tanto, las estad√≠sticas U de Theil mayores que 1 funcionan peor que un pron√≥stico
# ingenuo, un valor de 1 equivale a ingenuo y menos de 1 indica que el modelo tiene un
# rendimiento ingenuo. M√°s informaci√≥n sobre c√≥mo se deriva la estad√≠stica est√° disponible en
# este enlace:
#   https://www.researchgate.net/publication/323754973_Forecasting_methods_and_principles
# _Evidence-based_checklists_forecastingprinciplescom_forprincom
# El modelo de suavizado proporcion√≥ una estad√≠stica de 0.7940449. Eso no es muy
# impresionante a pesar de que est√° por debajo de uno. En la opini√≥n de los expertos,
# deber√≠amos esforzarnos por alcanzar valores iguales o inferiores a 0.5.
# Ahora desarrollaremos un modelo ARIMA, usando auto.arima(), que tambi√©n pertenece al
# paquete forecast.
# Hay muchas opciones que puedes especificar en la funci√≥n, o simplemente puedes incluir los
# datos de tu serie de tiempo y encontrar√°s el mejor ajuste ARIMA. Recomendamos utilizar la
# funci√≥n con precauci√≥n, ya que a menudo puede devolver un modelo que viola los supuestos
# para los residuos, como veremos:
fit.arima <- forecast::auto.arima(train)
summary(fit.arima)


# El resultado abreviado muestra que el modelo seleccionado es AR = 0, I = 1 y MA = 4, I = 1, o
# ARIMA(0,1,4). Podemos examinar el gr√°fico de su desempe√±o en los datos de prueba de la misma
# manera que antes:
plot(forecast::forecast(fit.arima, h = 6))
lines(test, type = "o")

# Esto es muy similar al m√©todo anterior. Revisemos esas estad√≠sticas de precisi√≥n, por
# supuesto centr√°ndonos en la U de Theil:
fit.arima %>% forecast::forecast(h = 6) %>% forecast::accuracy(temp_ts)

# El error de pron√≥stico es ligeramente mejor con el modelo ARIMA. Siempre debes revisar
# los residuos con tus modelos y especialmente ARIMA, que se basa en el supuesto de que no
# hay correlaci√≥n serial en dichos residuos:
forecast::checkresiduals(fit.arima)



# En primer lugar, echa un vistazo a la prueba Ljung-Box Q. La hip√≥tesis nula es que las
# correlaciones en los residuos son cero y la alternativa es que los residuos exhiben una
# correlaci√≥n serial. Vemos un valor p significativo por lo que podemos rechazar el valor nulo.
# Esto se confirma visualmente en el gr√°fico ACF de los residuos donde existe una correlaci√≥n
# significativa en el desfase 10 y el desfase 17. Con la correlaci√≥n serial presente, los
# coeficientes del modelo son insesgados, pero los errores est√°ndar y cualquier estad√≠stica que
# se base en ellos son incorrectos. Este hecho puede requerir que selecciones manualmente un
# modelo ARIMA apropiado mediante prueba y error. Explicar c√≥mo hacerlo requerir√≠a un
# documento extra, por lo que no est√° dentro del alcance de este primer documento.
# Con un par de modelos relativamente d√©biles, podemos probar otros m√©todos. Juntaremos
# los dos modelos reci√©n creados y agregaremos una red neuronal de alimentaci√≥n directa desde
# la funci√≥n nnetar() disponible en el paquete de forecast. No apilaremos los modelos,
# simplemente tomaremos el promedio de los tres modelos para compararlos con los datos de
# prueba.
# El primer paso en este proceso es desarrollar los pron√≥sticos para cada uno de los modelos.
# Esto es sencillo:
  
ETS <- forecast::forecast(forecast::ets(train), h = 6)
ARIMA <- forecast::forecast(forecast::auto.arima(train), h = 6)
NN <- forecast::forecast(forecast::nnetar(train), h = 6)




# El siguiente paso es crear los valores del conjunto, que nuevamente es solo un promedio
# simple:
ensemble.fit <- (ETS[["mean"]] + ARIMA[["mean"]] + NN[["mean"]]) / 3
# El paso de comparaci√≥n es una especie de lienzo abierto para que tu produzcas las estad√≠sticas
# que desees. Ten en cuenta que estamos obteniendo la precisi√≥n solo para los datos de prueba
# y la U de Theil. Puedes obtener las estad√≠sticas necesarias, como RMSE o MAPE, si as√≠ lo
# deseas:
c(ets = forecast::accuracy(ETS, temp_ts)["Test set", c("Theil's U")],
      arima = forecast::accuracy(ARIMA, temp_ts)["Test set", c("Theil's U")],
      nn = forecast::accuracy(NN, temp_ts)["Test set", c("Theil's U")],
      ef = forecast::accuracy(ensemble.fit, temp_ts)["Test set", c("Theil's U")])
# 
# Creo que esto es interesante, ya que los valores obtenidos son diferentes entre si, ets y ef son
# parecidos, casi iguales.
# Solo para una comparaci√≥n visual, tracemos la red neuronal:
plot(NN)
lines(test, type = "o")

# ¬øQu√© vamos a hacer con todo esto? Aqu√≠ hay un par de pensamientos. Si nos fijamos en el
# patr√≥n de las series temporales, observamos que pasa por lo que podr√≠amos llamar diferentes
# cambios estructurales. Hay varios paquetes de R para examinar esta estructura y determinar
# un punto en el que tiene m√°s sentido iniciar la serie temporal para la previsi√≥n. Por ejemplo,
# parece haber un cambio perceptible en la pendiente de la serie temporal a mediados de los
# a√±os sesenta. Cuando haces esto con tus datos, est√°s desperdiciando puntos de datos que
# podr√≠an ser valiosos, por lo que entra en juego el criterio. La implicaci√≥n es que, si deseas
# automatizar totalmente tus modelos de series temporales, deber√°s tener esto en cuenta.
# Puedes intentar transformar toda la serie temporal con valores logar√≠tmicos (esto no funciona
#                                                                              muy bien con valores negativos) o Box-Cox. En el paquete forecast, puedes configurar
# lambda = "auto", en la funci√≥n de tu modelo. Hice esto y el rendimiento no mejor√≥. A modo
# de ejemplo, intentemos detectar cambios estructurales y construir un modelo ARIMA en un
# punto de partida seleccionado.
# Demostraremos el cambio estructural con el paquete strucchange, que computacionalmente
# determina cambios en las relaciones de regresi√≥n lineal. Puedes encontrar una discusi√≥n
# completa y una vi√±eta sobre el paquete en este enlace:
#   https://cran.r-project.org/web/packages/strucchange/vignettes/strucchange-intro.pdf
# Encontramos que este m√©todo es √∫til en las discusiones con las partes interesadas, ya que les
# ayuda a comprender cu√°ndo e incluso por qu√© cambi√≥ el proceso subyacente de generaci√≥n
# de datos. Aqu√≠ va:
  
install.packages("strucchange")
library(strucchange)
temp_struc <- strucchange::breakpoints(temp_ts ~ 1)
summary(temp_struc)


# El algoritmo nos dio cinco posibles puntos de interrupci√≥n en la serie temporal, devolviendo
# la informaci√≥n como un n√∫mero de observaci√≥n y un a√±o. Efectivamente, 1963 indica un
# cambio estructural, pero nos dice que 1978 y 1996 tambi√©n califican.
# Sigamos la pausa de 1963 como inicio de nuestra serie temporal para un modelo ARIMA:
train_bp <- window(temp_ts, start = 1963, end = 2007)
fit.arima2 <- forecast::auto.arima(train_bp)
fit.arima2 %>% forecast::forecast(h = 6) %>%forecast::accuracy(temp_ts)

# Ah√≠ lo tienes: para nuestra sorpresa, el desempe√±o es incluso peor que un pron√≥stico ingenuo,
# pero al menos hemos explicado c√≥mo implementar esa metodolog√≠a.
# Con esto, hemos completado la construcci√≥n de un modelo de pron√≥stico univariado para las
# anomal√≠as de la temperatura de la superficie y ahora pasaremos a la siguiente tarea de ver si
# los niveles de CO2 causan estas anomal√≠as.


# Examinando la causalidad
# Para esta parte del documento, creemos que aqu√≠ es donde llega el momento y separaremos
# la causalidad de la mera correlaci√≥n; bueno, estad√≠sticamente hablando, al menos. Esta no es
# la primera vez que se aplica esta t√©cnica al problema. Triacca (2005) no encontr√≥ evidencia
# que sugiera que el CO2 atmosf√©rico de Granger causara las anomal√≠as en la temperatura de
# la superficie. Por otro lado, Kodra (2010) concluy√≥ que existe una relaci√≥n causal, pero
# advirti√≥ que sus datos no eran estacionarios incluso despu√©s de una diferenciaci√≥n de segundo
# orden.
# Si bien este esfuerzo no resolver√° el debate, es de esperar que te inspire a aplicar la
# metodolog√≠a en tus esfuerzos personales. El tema que nos ocupa ciertamente proporciona un
# campo de entrenamiento eficaz para demostrar la causalidad de Granger.
# Nuestro plan aqu√≠ es demostrar primero una regresi√≥n lineal espuria donde los residuos sufren
# de autocorrelaci√≥n, tambi√©n conocida como correlaci√≥n serial. Luego, examinaremos dos
# enfoques diferentes de la causalidad de Granger. Los primeros ser√°n los m√©todos
# tradicionales, donde ambas series son estacionarias. Luego, veremos el m√©todo demostrado
# por Toda y Yamamoto (1995), que aplica la metodolog√≠a a los datos brutos o, como a veces
# se les llama, los niveles (levels).
# 
# 
# Regresi√≥n lineal
# Comencemos entonces con la regresi√≥n espuria, que has visto implementada en el mundo
# real con demasiada frecuencia. Aqu√≠ simplemente construimos un modelo lineal y
# examinamos los resultados:
fit.lm <- lm(Temp ~ CO2, data = climate)
summary(fit.lm)


# Observe c√≥mo todo es significativo y tenemos una R-cuadrada ajustada de 0.7. Vale, est√°n
# muy correlacionados, pero nada de esto tiene sentido, como lo analizan Granger y Newbold
# (1974). Podemos trazar la correlaci√≥n serial, comenzando con una gr√°fica de serie temporal
# de los residuos, que produce un patr√≥n claro:
forecast::checkresiduals(fit.lm)


# Al examinar los gr√°ficos y la prueba de Breusch Godfrey, no sorprende que podamos
# rechazar con seguridad la hip√≥tesis nula de no autocorrelaci√≥n.
# La forma sencilla de abordar la autocorrelaci√≥n es incorporar variables rezagadas (lagged)
# de la serie temporal dependiente y/o hacer que todos los datos sean estacionarios. Lo haremos
# a continuaci√≥n utilizando vectores de auto regresi√≥n para identificar la estructura de retraso
# adecuada para incorporar en nuestros esfuerzos de causalidad. Uno de los puntos de cambio
# estructural fue 1949, as√≠ que comenzaremos por ah√≠.

# Autoregresi√≥n vectorial
# Hemos visto en la secci√≥n anterior que la temperatura y el CO2 requieren una diferencia de
# primer orden. Otra forma sencilla de mostrar esto es con la funci√≥n ndiffs() del paquete de
# pron√≥stico (forecast). Proporciona un resultado que detalla el n√∫mero m√≠nimo de diferencias
# necesarias para que los datos sean estacionarios. En la funci√≥n, puedes especificar qu√© prueba
# de las tres disponibles te gustar√≠a utilizar: Kwiatkowski, Philips, Schmidt & Shin (KPSS),
# Augmented DickeyFuller (ADF) o Philips-Peron (PP). Usaremos ADF en el siguiente
# c√≥digo, que tiene una hip√≥tesis nula de que los datos no son estacionarios:

climate49 <- window(climate_ts, start = 1949)
forecast::ndiffs(climate49[, 1], test = "adf")
forecast::ndiffs(climate49[, 2], test = "adf")

# Vemos que ambos requieren una diferencia de primer orden para volverse estacionarios. Para
# empezar, crearemos una diferencia. Luego, completaremos el m√©todo tradicional, donde
# ambas series son estacionarias:
climate_diff <- diff(climate49)

# Ahora es cuesti√≥n de determinar la estructura de retardo (lag) √≥ptima bas√°ndose en los
# criterios de informaci√≥n utilizando vectores autorregresivos. Esto se hace con la funci√≥n
# VARselect en el paquete vars. Solo necesitas especificar los datos y la cantidad de retrasos
# en el modelo usando lag.max = x en la funci√≥n. Utilicemos un m√°ximo de 12 rezagos (lags):

install.packages("vars")
library(vars)
lag.select <- vars::VARselect(climate_diff, lag.max = 12)
lag.select$selection

# Llamamos a los criterios de informaci√≥n usando lag$selection. Se proporcionan cuatro
# criterios diferentes, incluidos AIC, Criterio de Hannan-Quinn (HQ), Criterio de
# SchwarzBayes (SC) y FPE. Ten en cuenta que AIC y SC se tratan en regresi√≥n lineal
# com√∫nmente, por lo que no repasaremos aqu√≠ las f√≥rmulas de criterios ni las diferencias. Si
# deseas ver los resultados reales de cada retraso, puedes utilizar lag$criteria. Podemos ver que
# AIC y FPE han seleccionado el retraso 5 y HQ y SC el retraso 1 como la estructura √≥ptima
# para un modelo VAR. Parece tener sentido que se utilice el desfase de cinco a√±os. Crearemos
# ese modelo usando la funci√≥n var(). Te dejaremos probarlo con el retraso 1:

fit1 <- vars::VAR(climate_diff, p = 5)

# El resumen de resultados es bastante extenso ya que construye dos modelos separados y
# probablemente ocupar√≠a dos p√°ginas enteras. Lo que proporcionamos es el resultado
# abreviado que muestra los resultados con la temperatura como predicci√≥n:
summary(fit1)


# El modelo es significativo con un R-cuadrada ajustada resultante de 0.36.
# Como hicimos en la secci√≥n anterior, debemos verificar la correlaci√≥n serial.
# Aqu√≠, el paquete VAR proporciona la funci√≥n serial.test() para la autocorrelaci√≥n
# multivariada. Ofrece varias pruebas diferentes, pero centr√©monos en la prueba Portmanteau
# y ten en cuenta que la popular prueba Durbin-Watson es solo para series univariadas. La
# hip√≥tesis nula es que las autocorrelaciones son cero y la alternativa es que no son cero:
vars::serial.test(fit1, type = "PT.asymptotic")


# Con un valor p de 0.8794, no tenemos evidencia para rechazar el valor nulo y podemos decir
# que los residuos no est√°n autocorrelacionados. ¬øQu√© dice la prueba con 1 de retraso?
#   Para realizar las pruebas de causalidad de Granger en R, puedes utilizar el paquete lmtest y
# la funci√≥n Grangertest() o la funci√≥n causality() en el paquete vars.
# Demostraremos la t√©cnica usando causality(). Es muy f√°cil ya que s√≥lo necesitas crear dos
# objetos, uno para x que causa y y otro para y que causa x, utilizando el objeto fit1 creado
# previamente:
x2y <- vars::causality(fit1, cause = "CO2")
y2x <- vars::causality(fit1, cause = "Temp")
# Ahora es muy sencillo llamar a los resultados de la prueba de Granger:
x2y$Granger
y2x$Granger

# El valor p para las diferencias de CO2 de Granger que causan la temperatura es 0.02133 y no
# es significativo en la otra direcci√≥n. Entonces, ¬øQu√© significa todo esto? Lo primero que
# podemos decir es que Y no causa X. En cuanto a que X causa Y, podemos rechazar el valor
# nulo en el nivel de significancia 0.05 y, por lo tanto, concluir que X Granger causa Y. Sin
# embargo, ¬øes esa la conclusi√≥n relevante aqu√≠? Recuerda, el valor p eval√∫a la probabilidad
# del efecto si la hip√≥tesis nula es cierta. Adem√°s, recuerda que la prueba nunca fue dise√±ada
# para ser un s√≠ o un no binario.
# Dado que este estudio se basa en datos de observaci√≥n, creo que podemos decir que es muy
# probable que las emisiones de CO2 de Granger provoquen anomal√≠as en la temperatura de la
# superficie. Pero hay mucho margen de cr√≠tica sobre esa conclusi√≥n. Mencionamos desde el
# principio la controversia en torno a la calidad de los datos.
# Sin embargo, todav√≠a necesitamos modelar los niveles originales de CO2 utilizando la t√©cnica
# alternativa de causalidad de Granger. El proceso para encontrar el n√∫mero correcto de
# retrasos es el mismo que antes, excepto que no es necesario que los datos sean estacionarios:
level.select <- vars::VARselect(climate49, lag.max = 12)
level.select$selection


# Probemos la estructura de retardo 6 y veamos si podemos lograr significancia, recordando
# agregar un retardo adicional para tener en cuenta la serie integrada. Una discusi√≥n sobre la
# t√©cnica y por qu√© es necesario hacerlo est√° disponible en:
#   https://davegiles.blogspot.com/2011/04/testing-for-granger-causality.html
#   Ahora, para determinar la causalidad de Granger para que X cause Y, se realiza una prueba
# de Wald, donde los coeficientes de X y solo X son 0 en la ecuaci√≥n para predecir Y,
# recordando no incluir los coeficientes adicionales que explican la integraci√≥n en la prueba.
# La prueba de Wald en R est√° disponible en el paquete aod que tenemos que instalar y cargar.
# Necesitamos especificar los coeficientes del modelo completo, su matriz de covarianza y
# varianza y los coeficientes de la variable causante.
# Los coeficientes de Temp que necesitamos probar en el objeto VAR constan de un
# rango de n√∫meros pares de 2 a 12, mientras que los coeficientes de CO2 son impares
# de 1 a 11. En lugar de usar c(2, 4, 6, etc.) en nuestra funci√≥n, creemos un objeto con
# la funci√≥n seq() de la base de R.
# Primero, veamos c√≥mo el CO2 causa Granger la temperatura:
CO2terms <- seq(1, 11, 2)
Tempterms <- seq(2, 12, 2)
# Ahora estamos listos para ejecutar la prueba de Wald, que se describe en el siguiente c√≥digo
# y en el resultado abreviado:
aod::wald.test( b = coef(fit1$varresult$Temp),
    Sigma = vcov(fit1$varresult$Temp),
    Terms = c(CO2terms)
    )


# ¬øQu√© hay sobre eso? Tenemos un valor p significativo, as√≠ que probemos la causalidad en
# otra direcci√≥n con el siguiente c√≥digo:
aod::wald.test(
  b = coef(fit1$varresult$CO2),
  Sigma = vcov(fit1$varresult$CO2),
 Terms = c(Tempterms)
  )


# Por el contrario, podemos decir que la temperatura no causa Granger CO2. Lo √∫ltimo que se
# muestra aqu√≠ es c√≥mo utilizar un vector de auto regresi√≥n para producir un pron√≥stico. Hay
# una funci√≥n de predicci√≥n disponible y trazaremos el pron√≥stico para 24 a√±os:
plot(predict(fit1, n.ahead = 24, ci = 0.95))
