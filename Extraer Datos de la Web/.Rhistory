# Mostrar los nombres de las columnas
colnames(tayko_data)
# Crear una función para calcular promedio y desviación estándar
summary_stats <- function(data, var) {
data %>%
group_by(!!sym(var)) %>%
summarise(
mean_spending = mean(Spending, na.rm = TRUE),
sd_spending = sd(Spending, na.rm = TRUE)
)
}
# Aplicar la función a las variables categóricas
categorical_vars <- c("Web.order", "Gender.male", "Address_is_res", "US")
lapply(categorical_vars, summary_stats, data = tayko_data)
# b. Explorar la relación entre el gasto y cada uno de los predictores continuos
# Creamos diagramas de dispersión para Spending vs Freq y Spending vs LAST_UPDATE.
# Diagrama de dispersión para Spending vs Freq
ggplot(tayko_data, aes(x = Freq, y = Spending)) +
geom_point() +
geom_smooth(method = "lm", col = "red") +
labs(title = "Spending vs Freq", x = "Freq", y = "Spending")
# Diagrama de dispersión para Spending vs last_update_days_ago
ggplot(tayko_data, aes(x = last_update_days_ago, y = Spending)) +
geom_point() +
geom_smooth(method = "lm", col = "red") +
labs(title = "Spending vs last_update_days_ago", x = "last_update_days_ago", y = "Spending")
# c. Ajustar un modelo predictivo de gastos
# i. Particionar los datos en conjuntos de entrenamiento y validación
# Particionar los datos en conjuntos de entrenamiento y validación
library(caret)
library(lattice)
set.seed(123)
training_index <- createDataPartition(tayko_data$Spending, p = 0.7, list = FALSE)
training_data <- tayko_data[training_index, ]
validation_data <- tayko_data[-training_index, ]
# ii. Ejecutar un modelo de regresión lineal múltiple
# Ajustar el modelo de regresión lineal múltiple
model <- lm(Spending ~ Freq + last_update_days_ago + Web.order + Gender.male + Address_is_res + US, data = training_data)
summary(model)
# iii. Analizar el tipo de comprador que gasta más
# Mostrar los coeficientes del modelo
coefficients(model)
# iv. Eliminar predictores usando eliminación hacia atrás
# Eliminar predictores no significativos usando eliminación hacia atrás
step_model <- step(model, direction = "backward")
summary(step_model)
# v. Calcular predicción y error de predicción para la primera compra en el conjunto de validación
# Predicción para la primera compra en el conjunto de validación
first_pred <- predict(step_model, newdata = validation_data[1, ])
first_actual <- validation_data$Spending[1]
prediction_error <- first_actual - first_pred
# Mostrar predicción y error
list(prediction = first_pred, actual = first_actual, error = prediction_error)
# vi. Evaluar la precisión predictiva del modelo
# Evaluar el modelo en el conjunto de validación
predictions <- predict(step_model, newdata = validation_data)
validation_results <- data.frame(
actual = validation_data$Spending,
predicted = predictions,
residuals = validation_data$Spending - predictions
)
# Calcular el RMSE
rmse <- sqrt(mean(validation_results$residuals^2))
rmse
# vii. Crear un histograma de los residuos del modelo
# Crear histograma de los residuos
ggplot(validation_results, aes(x = residuals)) +
geom_histogram(binwidth = 5, fill = "blue", color = "black") +
labs(title = "Histograma de los Residuos", x = "Residuos", y = "Frecuencia")
# Evaluar si los residuos siguen una distribución normal
qqnorm(validation_results$residuals)
qqline(validation_results$residuals, col = "red")
shapiro.test(model$residuals)
plot(model,5)
# Distancia de cook
# ningun valor es influyente
# Esto es generalmente una situación favorable, ya que significa que el modelo
# es robusto a la presencia de valores atípicos o influyentes.
cooks.distance(model)
which(cooks.distance(model) > 1)
# ----------------------- alternativa -------------------
# Transformación logarítmica de la variable de respuesta Spending
tayko_data$log_Spending <- log(tayko_data$Spending + 1) # Agregar 1 para evitar log(0)
# Particionar los datos en conjuntos de entrenamiento y validación nuevamente
set.seed(123)
training_index <- createDataPartition(tayko_data$log_Spending, p = 0.7, list = FALSE)
training_data <- tayko_data[training_index, ]
validation_data <- tayko_data[-training_index, ]
# Ajustar el modelo de regresión lineal múltiple con la variable transformada
model_log <- lm(log_Spending ~ Freq + last_update_days_ago + Web.order + Gender.male + Address_is_res + US, data = training_data)
summary(model_log)
# Evaluar el modelo en el conjunto de validación
log_predictions <- predict(model_log, newdata = validation_data)
validation_results_log <- data.frame(
actual = validation_data$log_Spending,
predicted = log_predictions,
residuals = validation_data$log_Spending - log_predictions
)
# Calcular el RMSE para el modelo transformado
rmse_log <- sqrt(mean(validation_results_log$residuals^2))
rmse_log
# Crear QQ-plot para los residuos del modelo transformado
qqnorm(validation_results_log$residuals)
qqline(validation_results_log$residuals, col = "red")
# Crear histograma de los residuos del modelo transformado
ggplot(validation_results_log, aes(x = residuals)) +
geom_histogram(binwidth = 0.1, fill = "blue", color = "black") +
labs(title = "Histograma de los Residuos (Modelo Transformado)", x = "Residuos", y = "Frecuencia")
# Calcular el RMSE para el modelo transformado
rmse_log <- sqrt(mean(validation_results_log$residuals^2))
rmse_log
hist(model$residuals, color = "grey")s
plot(model)
plot(model_log)
plot(model_log)
plot(model)
santander <- read.csv("santander_prepd.csv")
# 2. Aquí cargamos el archivo, luego verificamos las dimensiones y examinamos una
# tabla de etiquetas de clientes:
library(readr)
santander <- read_csv("Desktop/UNIVERSITY/Servicio-Social/Data-Mining/Regresion Logistica/santander_prepd.csv")
dim(santander)
table(santander$y)
View(santander)
trainIndex <- caret::createDataPartition(santander$y, p = 0.8, list = FALSE)
train <- santander[trainIndex, ]
test <- santander[-trainIndex, ]
table(train$y)
table(test$y)
# identifica las características en el conjunto de datos train que tienen una varianza muy baja.
# Las características con una varianza muy baja pueden no ser informativas para su modelo y podrían eliminarse antes del entrenamiento.
train_zero <- caret::nearZeroVar(train, saveMetrics = TRUE)
View(train_zero)
table(train_zero$zeroVar)
# 6. Bien, una característica ahora tiene variación cero debido a la división y podemos
# eliminarla:
train <- train[, train_zero$zeroVar == 'FALSE']
# selecciona todas las filas y solo aquellas columnas
# donde el valor en zeroVar  es FALSE.
# solo conserva las columnas que no fueron identificadas como de baja varianza en el paso anterior
train <- train[, train_zero$zeroVar == 'FALSE']
library(tidyverse)
library(magrittr)
library(caret)
library(classifierplots)
install.packages("classifierplots")
# magrittr en nuestro entorno:
library(tidyverse)
library(magrittr)
library(caret)
library(classifierplots)
library(earth)
install.packages("earth")
library(tidyverse)
library(magrittr)
library(caret)
library(classifierplots)
library(earth)
library(Information)
install.packages("Information")
install.packages("Metrics")
library(tidyverse)
library(magrittr)
library(caret)
library(classifierplots)
library(earth)
library(Information)
library(Metrics)
# 2. Aquí cargamos el archivo, luego verificamos las dimensiones y examinamos una
# tabla de etiquetas de clientes:
library(readr)
santander <- read_csv("Desktop/UNIVERSITY/Servicio-Social/Data-Mining/Regresion Logistica/santander_prepd.csv")
dim(santander)
table(santander$y)
set.seed(1966)
trainIndex <- caret::createDataPartition(santander$y, p = 0.8, list = FALSE)
train <- santander[trainIndex, ]
test <- santander[-trainIndex, ]
# 4. Veamos cómo se equilibra la respuesta entre los dos conjuntos de datos:
table(train$y)
table(test$y)
# identifica las características en el conjunto de datos train que tienen una varianza muy baja.
# Las características con una varianza muy baja pueden no ser informativas para su modelo y podrían eliminarse antes del entrenamiento.
train_zero <- caret::nearZeroVar(train, saveMetrics = TRUE)
table(train_zero$zeroVar)
# selecciona todas las filas y solo aquellas columnas
# donde el valor en zeroVar  es FALSE.
# solo conserva las columnas que no fueron identificadas como de baja varianza en el paso anterior
train <- train[, train_zero$zeroVar == 'FALSE']
# Tomemos un ejemplo y demostremos en R.
# Nuestros datos constan de una característica de entrada codificada como 0 o 1, por lo que
# solo tendremos dos contenedores. Para cada contenedor (bin), calculamos nuestro WOE. En
# el contenedor 1, o donde los valores son iguales a 0, hay cuatro observaciones como eventos
# y 96 como no eventos. Por el contrario, en el grupo 2, o donde los valores son iguales a 1,
# tenemos 12 observaciones como eventos y 88 como no eventos. Veamos cómo calcular el
# WOE para cada contenedor:
bin1events <- 4
bin1nonEvents <- 96
bin2events <- 12
bin2nonEvents <- 88
totalEvents <- bin1events + bin2events
totalNonEvents <- bin1nonEvents + bin2nonEvents
# Now calculate the percentage per bin
bin1percentE <- bin1events / totalEvents
bin1percentNE <- bin1nonEvents / totalNonEvents
bin2percentE <- bin2events / totalEvents
bin2percentNE <- bin2nonEvents / totalNonEvents
# It's now possible to produce WOE
bin1WOE <- log(bin1percentE / bin1percentNE)
bin2WOE <- log(bin2percentE / bin2percentNE)
> bin1WOE
bin1WOE
bin2WOE
bin1IV <- (bin1percentE - bin1percentNE) * bin1WOE
bin2IV <- (bin2percentE - bin2percentNE) * bin2WOE
bin1IV + bin2IV
IV <- Information::create_infotables(data = train, y = "y", parallel = FALSE)
IV <- Information::create_infotables(data = train, y = "y", parallel = FALSE)
# Esto nos dará un resumen IV de las 25 características principales:
knitr::kable(head(IV$Summary, 25))
View(train)
# Los resultados nos muestran el número de columna de la característica, el nombre de la
# característica y el IV.
# Observa que tenemos cinco características que posiblemente sean sospechosas. Estoy
# totalmente a favor de tomar cualquier característica con un IV superior a 0.02, que es la parte
# inferior de los predictores débiles. Eso nos dará 21 funciones de entrada. La característica V2
# es interesante. Si nos fijamos en los valores y pensamos en los datos, parece claro que se trata
# de la edad del cliente. Veamos cómo se agrupan los datos, los valores WOE y los IV:
knitr::kable(IV$Tables$V2)
Information::plot_infotables(IV, "V2", show_values = TRUE)
# Es interesante que exista una relación algo lineal entre esta característica y la respuesta. Lo
# que se puede hacer es crear funciones que conviertan los valores agrupados en valores WOE.
# Estas nuevas funciones serían lineales y podrían usarse en lugar de las funciones originales.
# Renunciaremos a eso porque ¿qué método hará eso por nosotros? Así es, ¡MARS
# puede hacer eso por nosotros! Aquí hay un diagrama de cuadrícula de las
# cuatro características principales:
Information::plot_infotables(IV, IV$Summary$Variable[1:4], same_scales=TRUE)
features <- IV$Summary$Variable[1:21]
train_reduced <- train[, colnames(train) %in% features]
train_reduced$y <- train$y
glm_control <-caret::trainControl(method = "cv",number = 5,returnResamp = "final")
x <- train_reduced[, -22]
y <- as.factor(train_reduced$y)
set.seed(1988)
glm_fit <-caret::train(x, y, method = "glm",
trControl = glm_control,
trace = FALSE)
Cuando hayas terminado, podrás comprobar rápidamente los resultados:
# Cuando hayas terminado, podrás comprobar rápidamente los resultados:
glm_fit$results
glm_train_pred <- predict(glm_fit, train, type = "prob")
colnames(glm_train_pred) <- c("zero", "one")
classifierplots::density_plot(train_reduced$y, glm_train_pred$one)
glm_train_pred <- predict(glm_fit, train, type = "prob")
colnames(glm_train_pred) <- c("zero", "one")
classifierplots::density_plot(train_reduced$y, glm_train_pred$one)
# Dado el desequilibrio en las clases, se trata de una enorme cantidad de clientes. Una matriz
# de confusión puede demostrar ese hecho:
InformationValue::confusionMatrix(train_reduced$y, glm_train_pred$one, threshold = 0.0607)
install.packages("Information")
install.packages("Information")
library(InformationValue)
install.packages("InformationValue")
library(InformationValue)
install.packages("~/Downloads/InformationValue/inst/doc/InformationValue.R", repos = NULL)
glm_test_pred <- predict(glm_fit, test, type = "prob")
colnames(glm_test_pred) <- c("zero", "one")
classifierplots::density_plot(test$y, glm_test_pred$one)
# Resultados muy similares en los datos de la prueba. ¿Qué pasa con una matriz de confusión
# dado nuestro umbral determinado durante el entrenamiento? Vamos a ver:
InformationValue::confusionMatrix(test$y, glm_test_pred$one, threshold = 0.0607)
Metrics::auc(test$y, glm_test_pred$one)
Metrics::logLoss(test$y, glm_test_pred$one)
set.seed(1972)
earth_fit <-earth::earth(x = train[, -142],
y = train[, 142],
pmethod = 'cv',
nfold = 5,
degree = 1,
minspan = -1,
nprune = 15,
glm = list(family = binomial)
)
summary(earth_fit)
# Ahora, especificamos la validación cruzada, pero earth simultáneamente selecciona y elimina
# características hacia adelante y hacia atrás, utilizando la validación cruzada generalizada
# (GCV, Generalized Cross-Validation). Por lo tanto, los resultados de GCV y rss para una
# característica se normalizan de 0 a 100 con fines de comparación.
# Como hicimos anteriormente, una gráfica de las densidades de probabilidad es útil y earth
# viene con su propia función plotd():
earth::plotd(earth_fit)
pred <- predict(earth_fit, train, type = 'response')
mars_cutoff <-InformationValue::optimalCutoff(
train$y,
pred,
optimiseFor = 'Both',
returnDiagnostics = TRUE)
install.packages("forecast")
library(forecast)
set.seed(1966)
ar1 <- arima.sim(list(order = c(1, 0, 0), ar = 0.5), n = 200)
forecast::autoplot(ar1, main = "AR1")
install.packages("forecast")
# Ahora, examinemos ACF:
forecast::autoplot(acf(ar1, plot = F), main = "ACF of simulated AR1")
forecast::autoplot(pacf(ar1, plot = F), main = "PACF of simulated AR1")
set.seed(123)
ma1 <- arima.sim(list(order = c(0, 0, 1), ma = -0.5), n = 200)
forecast::autoplot(ma1, main = "MA1")
# La figura anterior es el gráfico ACF y ahora veremos el gráfico PACF:
forecast::autoplot(pacf(ma1, plot = F), main = "PACF of simulated MA1")
setwd("/Users/jonatanbadillo/Desktop/UNIVERSITY/Servicio-Social/Data-Mining/Extraer Datos de la Web")
library(tidyverse)
library(rvest)
library(GGally)
install.packages("GGally")
library(tidyverse)
library(rvest)
library(GGally)
swgoh_url <- "https://swgoh.gg/characters/stats/"
stats <- read_html(swgoh_url) %>%
html_table() %>%
purrr::pluck(1)
swgoh_url <- "https://calidaddelaire.puebla.gob.mx/views/reporteICA.php"
stats <- read_html(swgoh_url) %>%
html_table() %>%
purrr::pluck(1)
glimpse(stats)
View(stats)
library(rvest)
library(dplyr)
library(httr)
# URL de la página
url <- "https://calidaddelaire.puebla.gob.mx/views/reporteICA.php"
# Simulación de un formulario POST para obtener los datos específicos
response <- POST(url, body = list(
fecha = "01/01/2023",
estacion = "BINE",
tipo_consulta = "estacion"
))
# Leer el contenido de la respuesta
web_content <- content(response, as = "text", encoding = "UTF-8")
# Parsear el contenido HTML
page <- read_html(web_content)
# Extraer la tabla de datos
table <- page %>%
html_node("table") %>%
html_table(fill = TRUE)
# Mostrar la tabla
print(table)
View(table)
View(stats)
View(page)
View(stats)
View(response)
View(table)
View(page)
library(rvest)
library(dplyr)
library(httr)
# URL de la página
url <- "https://calidaddelaire.puebla.gob.mx/views/reporteICA.php"
# Simulación de un formulario POST para obtener los datos específicos
response <- POST(url, body = list(
fecha = "01/01/2023",
estacion = "BINE",
tipo_consulta = "estacion"
))
# Leer el contenido de la respuesta
web_content <- content(response, as = "text", encoding = "UTF-8")
# Parsear el contenido HTML
page <- read_html(web_content)
# Extraer la tabla de datos
table <- page %>%
html_node("table") %>%
html_table(fill = TRUE)
# Mostrar la tabla
print(table)
View(table)
View(response)
View(page)
library(rvest)
library(tidyverse)
library(gganimate)
install.packages("gganimate")
library(rvest)
library(tidyverse)
library(gganimate)
library(RColorBrewer)
library(kableExtra)
install.packages("kableExtra")
library(rvest)
library(tidyverse)
library(gganimate)
library(RColorBrewer)
library(kableExtra)
baseUrl <- "https://www.weltfussball.de/"
path <- "spielplan/eng-premier-league-2018-2019-spieltag/"
fileName <- 1
url <- paste0(baseUrl, path, fileName)
url
library(rvest)
library(tidyverse)
library(gganimate)
library(RColorBrewer)
library(kableExtra)
baseUrl <- "https://www.weltfussball.de/"
path <- "spielplan/eng-premier-league-2018-2019-spieltag/"
fileName <- 1
url <- paste0(baseUrl, path, fileName)
url
# Extraer las horas
nombres <- pagina %>% html_nodes("#div_historial_ICA > div > table > tbody > tr:nth-child(1) > td:nth-child(1)")%%
html_text()
#url de la pagina
url <- "https://calidaddelaire.puebla.gob.mx/views/reporteICA.php"
pagina <- read_html(url)
# Extraer las horas
nombres <- pagina %>% html_nodes("#div_historial_ICA > div > table > tbody > tr:nth-child(1) > td:nth-child(1)")%%
html_text()
# Extraer las horas
nombres <- pagina %>% html_nodes("#div_historial_ICA > div > table > tbody > tr:nth-child(1) > td:nth-child(1)")%>%
html_text()
#url de la pagina
url <- "https://calidaddelaire.puebla.gob.mx/views/reporteICA.php"
pagina <- read_html(url)
# Extraer las horas
nombres <- pagina %>% html_nodes("#div_historial_ICA > div > table > tbody > tr:nth-child(1) > td:nth-child(1)")%>%
html_text()
nombres
# Importacion de librerias necesarias
library(dplyr)
library(rvest)
#url de la pagina
url <- "https://calidaddelaire.puebla.gob.mx/views/reporteICA.php"
pagina <- read_html(url)
# Extraer las horas
nombres <- pagina %>% html_nodes("#div_historial_ICA > div > table > tbody > tr:nth-child(2) > td:nth-child(1)")%>%
html_text()
nombres
# Importacion de librerias necesarias
library(dplyr)
library(rvest)
library(tidyverse)
#url de la pagina
url <- "https://calidaddelaire.puebla.gob.mx/views/reporteICA.php"
pagina <- read_html(url)
pagina   %>% html_text()
pagina   %>% html_nodes(".align-middle")
pagina %>% html_nodes(".align-middle") %>% html_text()
pagina %>% html_nodes(".table-container") %>% html_text()
pagina %>% html_nodes("p") %>% html_text()
pagina %>% html_nodes("p") %>% html_text()
pagina %>% html_nodes("table table-bordered") %>% html_text()
pagina %>% html_nodes(".table table-bordered") %>% html_text()
pagina %>% html_nodes(".table .table-bordered") %>% html_text()
pagina %>% html_nodes(".table ") %>% html_text()
table <- pagina %>% html_table()
View(table)
table[[2]][["Estación"]]
class(table)
table <- pagina %>% html_table()
class(table)
View(table)
pagina %>% html_nodes('#div_historial_ICA > div > table') %>% html_text()
pagina %>% html_nodes('#div_historial_ICA > div > table') %>% html_table()
table <- pagina %>% html_nodes('#div_historial_ICA > div > table') %>% html_table()
table <- pagina %>% html_table('#div_historial_ICA > div > table')
# Extraer la tabla
tabla <- html_nodes(pagina, "table") %>%
html_table(fill = TRUE)
# Mostrar la tabla
print(tabla)
# Leer la segunda tabla de la página web
tabla <- html_nodes(pagina, xpath = '//*[@class="table table-bordered"][2]') %>%
html_table(fill = TRUE)
# Mostrar la tabla
print(tabla)
# Leer la tabla dentro del elemento .modal-content
tabla <- html_nodes(pagina, xpath = '//div[@class="modal-content"]//table') %>%
html_table(fill = TRUE)
# Mostrar la tabla
print(tabla)
# Leer la tabla dentro del elemento con id div_historial_ICA
tabla <- html_nodes(pagina, xpath = '//*[@id="div_historial_ICA"]//table') %>%
html_table(fill = TRUE)
# Mostrar la tabla
print(tabla)
# Extraer la tabla
tabla <- html_nodes(pagina, "table") %>%
html_table(fill = TRUE)
View(tabla)
# Leer la tabla dentro del elemento con id div_historial_ICA
tabla <- html_nodes(pagina, xpath = '//*[(@id = "div_historial_ICA")]') %>%
html_table(fill = TRUE)
# Mostrar la tabla
print(tabla)
# Leer la tabla utilizando el XPath proporcionado
tabla <- html_nodes(pagina, xpath = '//*[(@id = "div_historial_ICA")]//table') %>%
html_table(fill = TRUE)
# Mostrar la tabla
print(tabla)
table <- pagina %>% html_table('#div_historial_ICA > div > table')
table <- pagina %>% html_nodes('#div_historial_ICA > div > table') %>% html_table()
pagina %>% html_table()
table <- pagina %>% html_table()
# Leer la tabla utilizando el XPath proporcionado
tabla <- html_nodes(pagina, xpath = '//*[(@id = "td_ICA_CO_0")] | //*[contains(concat( " ", @class, " " ), concat( " ", "align-middle", " " )) and (((count(preceding-sibling::*) + 1) = 6) and parent::*)]') %>%
html_table(fill = TRUE)
# Mostrar la tabla
print(tabla)
table <-pagina %>% html_table()
