# Regresi贸n log铆stica
# Nuestra tarea es
# predecir la probabilidad de que una observaci贸n pertenezca a una categor铆a 
# particular de la variable de resultado. 
# En otras palabras, desarrollamos un algoritmo para clasificar las
# observaciones.


# M茅todos de clasificaci贸n y regresi贸n lineal

# 驴por qu茅 no podemos utilizar el m茅todo de regresi贸n de m铆nimos cuadrados que
#  para obtener un resultado cualitativo? Bueno, resulta que puedes,
# pero bajo tu propio riesgo. Supongamos por un segundo que tienes un resultado que est谩s
# tratando de predecir y que tienes tres clases diferentes: leve, moderado y severo. Tu y tus
# colegas tambi茅n suponen que la diferencia entre leve y moderado y moderado y grave es una
# medida equivalente y una relaci贸n lineal. Puedes crear una variable ficticia donde 0 es igual
# a leve, 1 es igual a moderado y 2 es igual a grave. Si tienes motivos para creer esto, entonces
# la regresi贸n lineal podr铆a ser una soluci贸n aceptable. Sin embargo, etiquetas cualitativas 
# como las anteriores podr铆an prestarse a un alto nivel de error de medici贸n que puede sesgar
# el OLS.


# En la mayor铆a de los problemas empresariales, no existe una forma cient铆ficamente aceptable
# de convertir una respuesta cualitativa en una cuantitativa. 驴Qu茅 pasa si tienes una respuesta
# con dos resultados, digamos reprobar y aprobar? Nuevamente, utilizando el enfoque de
# variable ficticia, podr铆amos codificar el resultado fallido como 0 y el resultado aprobado
# como 1. Usando la regresi贸n lineal, podr铆amos construir un modelo donde el valor predicho
# sea la probabilidad de una observaci贸n de aprobado o reprobado. Sin embargo, las
# estimaciones de Y en el modelo probablemente exceder谩n las restricciones de probabilidad
# de [0,1] y, por lo tanto, ser谩n un poco dif铆ciles de interpretar.



# Regresi贸n log铆stica
# Como sabemos de los m茅todos estudiado previamente, nuestro problema de clasificaci贸n se
# modela mejor con las probabilidades ligadas por 0 y 1. Podemos hacer esto para todas
# nuestras observaciones con algunas funciones diferentes, pero aqu铆 nos centraremos en la
# funci贸n log铆stica. La funci贸n log铆stica utilizada en la regresi贸n log铆stica es la siguiente:

#   Probabilidad de  =((0+1) / 1) +0+1

# Si alguna vez has realizado una apuesta amistosa en carreras de caballos o en la Copa del
# Mundo, es posible que comprendas mejor el concepto de probabilidades.
# La funci贸n log铆stica se puede convertir en probabilidades con la formulaci贸n de
# Probabilidad(Y)/1 - Probabilidad (Y).
# Por ejemplo, si la probabilidad de que Brasil gane la Copa del Mundo es del 20 por ciento,
# entonces las probabilidades son 0.2/1 - 0,2, lo que equivale a 0.25, lo que se traduce en
# probabilidades de uno entre cuatro.
# Para convertir las odds (probabilidades) en probabilidad, toma las odds y div铆delas por uno
# m谩s las probabilidades. Por lo tanto, el ejemplo de la Copa del Mundo es 0.25/1 + 0.25, lo
# que equivale al 20 por ciento. Adem谩s, consideremos la raz贸n de las odds. Supongamos que
# las odds de que Alemania gane la Copa son 0.18. Podemos comparar las odds de Brasil y
# Alemania con la raz贸n de las odds. En este ejemplo, la raz贸n de las odds ser铆a la de Brasil
# dividida por la de Alemania. Terminaremos con una raz贸n de probabilidades igual a
# 0.25/0.18, que es igual a 1.39. Aqu铆 diremos que Brasil tiene 1.39 veces m谩s probabilidades
# que Alemania de ganar la Copa del Mundo.


# Teniendo estos hechos en mente, la regresi贸n log铆stica es una t茅cnica potente para predecir
# los problemas que involucran clasificaci贸n y, a menudo, es el punto de partida para la
# creaci贸n de modelos en dichos problemas. 


# Formaci贸n y evaluaci贸n de modelos

# Predeciremos la satisfacci贸n del cliente. Los datos se
# basan en un antiguo concurso online. Tomamos la parte de entrenamiento de los datos y la
# limpiamos para nuestro uso (Santander customer satisfaction).
# Este es un conjunto de datos excelente para un problema de clasificaci贸n por muchas razones.
# Como muchos datos de clientes, es muy confuso, especialmente antes de que eliminaramos
# un mont贸n de caracter铆sticas in煤tiles (hab铆a algo as铆 como cuatro docenas de funciones de
# variaci贸n cero). Como se analiz贸 en termas previos anteriores, abordamos los valores
# faltantes, las dependencias lineales y los pares altamente correlacionados. Tambi茅n
# encontramos que los nombres de las funciones eran largos e in煤tiles, as铆 que los codificamos
# de V1 a V142. Los datos resultantes abordan lo que normalmente es dif铆cil de medir: la
# satisfacci贸n. Debido a los m茅todos patentados, no se proporciona ninguna descripci贸n o
# definici贸n de satisfacci贸n.
# El problema cl谩sico es que se terminan con
# bastantes falsos positivos al intentar clasificar las etiquetas minoritarias.
# Entonces, comencemos cargando los datos y entrenando un algoritmo de regresi贸n log铆stica.


# Entrenamiento de un algoritmo de regresi贸n log铆stica
# Sigue estos sencillos pasos para entrenar un algoritmo de regresi贸n log铆stica:
#   1. El primer paso es asegurarnos de cargar nuestros paquetes y llamar a la biblioteca
# magrittr en nuestro entorno:
library(tidyverse)
library(magrittr)
library(caret)
library(classifierplots)
library(earth)
library(Information)
library(Metrics)

# 2. Aqu铆 cargamos el archivo, luego verificamos las dimensiones y examinamos una
# tabla de etiquetas de clientes:
library(readr)
santander <- read_csv("Desktop/UNIVERSITY/Servicio-Social/Data-Mining/Regresion Logistica/santander_prepd.csv")
dim(santander)

table(santander$y)

# Tenemos 76,020 observaciones, pero s贸lo 3,008 clientes est谩n etiquetados con 1, lo
# que significa insatisfecho. A continuaci贸n, usaremos el s铆mbolo de intercalaci贸n para
# crear conjuntos de entrenamiento y prueba con una divisi贸n 80/20.

# 3. Dentro de la funci贸n createDataPartition() de caret, estratifica autom谩ticamente la
# muestra seg煤n la respuesta, por lo que podemos estar seguros de tener un porcentaje
# equilibrado entre el entrenamiento y los conjuntos de prueba:

set.seed(1966)
trainIndex <- caret::createDataPartition(santander$y, p = 0.8, list = FALSE)
train <- santander[trainIndex, ]
test <- santander[-trainIndex, ]

# 4. Veamos c贸mo se equilibra la respuesta entre los dos conjuntos de datos:
table(train$y)

table(test$y)

# Hay aproximadamente un 4 por ciento en cada conjunto, as铆 que podemos continuar.
# Una cosa interesante que puede suceder cuando divides los datos es que ahora
# terminas con lo que era una caracter铆stica de varianza casi cero convirti茅ndose en una
# caracter铆stica de varianza cero en tu conjunto de entrenamiento. Cuando trates estos
# datos, solo elimina las caracter铆sticas de varianza cero.


# 5. Hubo algunas caracter铆sticas de baja variaci贸n, as铆 que veamos si podemos eliminar
# algunas nuevas de variaci贸n cero:

# identifica las caracter铆sticas en el conjunto de datos train que tienen una varianza muy baja.
# Las caracter铆sticas con una varianza muy baja pueden no ser informativas para su modelo y podr铆an eliminarse antes del entrenamiento.
train_zero <- caret::nearZeroVar(train, saveMetrics = TRUE)
table(train_zero$zeroVar)


# 6. Bien, una caracter铆stica ahora tiene variaci贸n cero debido a la divisi贸n y podemos
# eliminarla:

# selecciona todas las filas y solo aquellas columnas 
# donde el valor en zeroVar  es FALSE. 
# solo conserva las columnas que no fueron identificadas como de baja varianza en el paso anterior
train <- train[, train_zero$zeroVar == 'FALSE']

# Como hicimos con la regresi贸n lineal, para que la regresi贸n log铆stica tenga
# resultados significativos, es decir, que no se sobreajuste, es necesario reducir la cantidad de
# caracter铆sticas de entrada. Podr铆amos seguir adelante con una selecci贸n gradual o similar,
# como hicimos en el tema de regresi贸n lineal. Podr铆amos implementar m茅todos de
# regularizaci贸n de caracter铆sticas. Sin embargo, queremos presentar un m茅todo de reducci贸n
# de caracter铆sticas univariadas utilizando el peso de la evidencia (WOE, Weight Of Evidence)
# y el valor de la informaci贸n (IV, Information Value) y discutir c贸mo podemos comprender
# c贸mo usarlo en un problema de clasificaci贸n junto con la regresi贸n log铆stica.


# Peso de la evidencia y valor de la informaci贸n
# 
# Dada la posibilidad de cientos, incluso miles, de caracter铆sticas posibles, tuvimos que
# aprender el uso de WOE y IV. Ahora bien, este m茅todo no es una panacea. En primer lugar,
# es univariado, por lo que las caracter铆sticas que se descartan pueden volverse significativas
# en un modelo multivariado y viceversa. Podemos decir que proporciona un buen
# complemento a otros m茅todos y deber铆as tenerlo en tu caja de herramientas de modelado.
# Creo que tuvo su origen en el mundo de la calificaci贸n crediticia, por lo que, si trabajas en la
# industria financiera, es posible que ya est茅s familiarizado con 茅l.
# Primero, veamos la f贸rmula de WOE:
#    = ln (porcentaje de eventos)/ (porcentaje de no eventos)

# El WOE sirve como componente del IV. Para funciones num茅ricas, agrupar铆a tus datos y
# luego calcular铆as WOE por separado para cada contenedor. Para los categ贸ricos, o cuando
# est谩n codificados en caliente, agrupa para cada nivel y calcula el WOE por separado. 




# Tomemos un ejemplo y demostremos en R.
# Nuestros datos constan de una caracter铆stica de entrada codificada como 0 o 1, por lo que
# solo tendremos dos contenedores. Para cada contenedor (bin), calculamos nuestro WOE. En
# el contenedor 1, o donde los valores son iguales a 0, hay cuatro observaciones como eventos
# y 96 como no eventos. Por el contrario, en el grupo 2, o donde los valores son iguales a 1,
# tenemos 12 observaciones como eventos y 88 como no eventos. Veamos c贸mo calcular el
# WOE para cada contenedor:
bin1events <- 4
bin1nonEvents <- 96
bin2events <- 12
bin2nonEvents <- 88
totalEvents <- bin1events + bin2events
totalNonEvents <- bin1nonEvents + bin2nonEvents
# Now calculate the percentage per bin
bin1percentE <- bin1events / totalEvents
bin1percentNE <- bin1nonEvents / totalNonEvents
bin2percentE <- bin2events / totalEvents
bin2percentNE <- bin2nonEvents / totalNonEvents
# It's now possible to produce WOE
bin1WOE <- log(bin1percentE / bin1percentNE)
bin2WOE <- log(bin2percentE / bin2percentNE)
bin1WOE
bin2WOE
