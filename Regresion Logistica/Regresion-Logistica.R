# Regresi√≥n log√≠stica
# Nuestra tarea es
# predecir la probabilidad de que una observaci√≥n pertenezca a una categor√≠a 
# particular de la variable de resultado. 
# En otras palabras, desarrollamos un algoritmo para clasificar las
# observaciones.


# M√©todos de clasificaci√≥n y regresi√≥n lineal

# ¬øpor qu√© no podemos utilizar el m√©todo de regresi√≥n de m√≠nimos cuadrados que
#  para obtener un resultado cualitativo? Bueno, resulta que puedes,
# pero bajo tu propio riesgo. Supongamos por un segundo que tienes un resultado que est√°s
# tratando de predecir y que tienes tres clases diferentes: leve, moderado y severo. Tu y tus
# colegas tambi√©n suponen que la diferencia entre leve y moderado y moderado y grave es una
# medida equivalente y una relaci√≥n lineal. Puedes crear una variable ficticia donde 0 es igual
# a leve, 1 es igual a moderado y 2 es igual a grave. Si tienes motivos para creer esto, entonces
# la regresi√≥n lineal podr√≠a ser una soluci√≥n aceptable. Sin embargo, etiquetas cualitativas 
# como las anteriores podr√≠an prestarse a un alto nivel de error de medici√≥n que puede sesgar
# el OLS.


# En la mayor√≠a de los problemas empresariales, no existe una forma cient√≠ficamente aceptable
# de convertir una respuesta cualitativa en una cuantitativa. ¬øQu√© pasa si tienes una respuesta
# con dos resultados, digamos reprobar y aprobar? Nuevamente, utilizando el enfoque de
# variable ficticia, podr√≠amos codificar el resultado fallido como 0 y el resultado aprobado
# como 1. Usando la regresi√≥n lineal, podr√≠amos construir un modelo donde el valor predicho
# sea la probabilidad de una observaci√≥n de aprobado o reprobado. Sin embargo, las
# estimaciones de Y en el modelo probablemente exceder√°n las restricciones de probabilidad
# de [0,1] y, por lo tanto, ser√°n un poco dif√≠ciles de interpretar.



# Regresi√≥n log√≠stica
# Como sabemos de los m√©todos estudiado previamente, nuestro problema de clasificaci√≥n se
# modela mejor con las probabilidades ligadas por 0 y 1. Podemos hacer esto para todas
# nuestras observaciones con algunas funciones diferentes, pero aqu√≠ nos centraremos en la
# funci√≥n log√≠stica. La funci√≥n log√≠stica utilizada en la regresi√≥n log√≠stica es la siguiente:

#   Probabilidad de ùëå =((ùëí0+ùõΩ1ùë•) / 1) +ùëíùõΩ0+ùõΩ1ùë•

# Si alguna vez has realizado una apuesta amistosa en carreras de caballos o en la Copa del
# Mundo, es posible que comprendas mejor el concepto de probabilidades.
# La funci√≥n log√≠stica se puede convertir en probabilidades con la formulaci√≥n de
# Probabilidad(Y)/1 - Probabilidad (Y).
# Por ejemplo, si la probabilidad de que Brasil gane la Copa del Mundo es del 20 por ciento,
# entonces las probabilidades son 0.2/1 - 0,2, lo que equivale a 0.25, lo que se traduce en
# probabilidades de uno entre cuatro.
# Para convertir las odds (probabilidades) en probabilidad, toma las odds y div√≠delas por uno
# m√°s las probabilidades. Por lo tanto, el ejemplo de la Copa del Mundo es 0.25/1 + 0.25, lo
# que equivale al 20 por ciento. Adem√°s, consideremos la raz√≥n de las odds. Supongamos que
# las odds de que Alemania gane la Copa son 0.18. Podemos comparar las odds de Brasil y
# Alemania con la raz√≥n de las odds. En este ejemplo, la raz√≥n de las odds ser√≠a la de Brasil
# dividida por la de Alemania. Terminaremos con una raz√≥n de probabilidades igual a
# 0.25/0.18, que es igual a 1.39. Aqu√≠ diremos que Brasil tiene 1.39 veces m√°s probabilidades
# que Alemania de ganar la Copa del Mundo.


# Teniendo estos hechos en mente, la regresi√≥n log√≠stica es una t√©cnica potente para predecir
# los problemas que involucran clasificaci√≥n y, a menudo, es el punto de partida para la
# creaci√≥n de modelos en dichos problemas. 


# Formaci√≥n y evaluaci√≥n de modelos

# Predeciremos la satisfacci√≥n del cliente. Los datos se
# basan en un antiguo concurso online. Tomamos la parte de entrenamiento de los datos y la
# limpiamos para nuestro uso (Santander customer satisfaction).
# Este es un conjunto de datos excelente para un problema de clasificaci√≥n por muchas razones.
# Como muchos datos de clientes, es muy confuso, especialmente antes de que eliminaramos
# un mont√≥n de caracter√≠sticas in√∫tiles (hab√≠a algo as√≠ como cuatro docenas de funciones de
# variaci√≥n cero). Como se analiz√≥ en termas previos anteriores, abordamos los valores
# faltantes, las dependencias lineales y los pares altamente correlacionados. Tambi√©n
# encontramos que los nombres de las funciones eran largos e in√∫tiles, as√≠ que los codificamos
# de V1 a V142. Los datos resultantes abordan lo que normalmente es dif√≠cil de medir: la
# satisfacci√≥n. Debido a los m√©todos patentados, no se proporciona ninguna descripci√≥n o
# definici√≥n de satisfacci√≥n.
# El problema cl√°sico es que se terminan con
# bastantes falsos positivos al intentar clasificar las etiquetas minoritarias.
# Entonces, comencemos cargando los datos y entrenando un algoritmo de regresi√≥n log√≠stica.


# Entrenamiento de un algoritmo de regresi√≥n log√≠stica
# Sigue estos sencillos pasos para entrenar un algoritmo de regresi√≥n log√≠stica:
#   1. El primer paso es asegurarnos de cargar nuestros paquetes y llamar a la biblioteca
# magrittr en nuestro entorno:
library(tidyverse)
library(magrittr)
library(caret)
library(classifierplots)
library(earth)
library(Information)
library(Metrics)

# 2. Aqu√≠ cargamos el archivo, luego verificamos las dimensiones y examinamos una
# tabla de etiquetas de clientes:
library(readr)
santander <- read_csv("Desktop/UNIVERSITY/Servicio-Social/Data-Mining/Regresion Logistica/santander_prepd.csv")
dim(santander)

table(santander$y)

# Tenemos 76,020 observaciones, pero s√≥lo 3,008 clientes est√°n etiquetados con 1, lo
# que significa insatisfecho. A continuaci√≥n, usaremos el s√≠mbolo de intercalaci√≥n para
# crear conjuntos de entrenamiento y prueba con una divisi√≥n 80/20.

# 3. Dentro de la funci√≥n createDataPartition() de caret, estratifica autom√°ticamente la
# muestra seg√∫n la respuesta, por lo que podemos estar seguros de tener un porcentaje
# equilibrado entre el entrenamiento y los conjuntos de prueba:

set.seed(1966)
trainIndex <- caret::createDataPartition(santander$y, p = 0.8, list = FALSE)
train <- santander[trainIndex, ]
test <- santander[-trainIndex, ]

# 4. Veamos c√≥mo se equilibra la respuesta entre los dos conjuntos de datos:
table(train$y)

table(test$y)

# Hay aproximadamente un 4 por ciento en cada conjunto, as√≠ que podemos continuar.
# Una cosa interesante que puede suceder cuando divides los datos es que ahora
# terminas con lo que era una caracter√≠stica de varianza casi cero convirti√©ndose en una
# caracter√≠stica de varianza cero en tu conjunto de entrenamiento. Cuando trates estos
# datos, solo elimina las caracter√≠sticas de varianza cero.


# 5. Hubo algunas caracter√≠sticas de baja variaci√≥n, as√≠ que veamos si podemos eliminar
# algunas nuevas de variaci√≥n cero:

# identifica las caracter√≠sticas en el conjunto de datos train que tienen una varianza muy baja.
# Las caracter√≠sticas con una varianza muy baja pueden no ser informativas para su modelo y podr√≠an eliminarse antes del entrenamiento.
train_zero <- caret::nearZeroVar(train, saveMetrics = TRUE)
table(train_zero$zeroVar)


# 6. Bien, una caracter√≠stica ahora tiene variaci√≥n cero debido a la divisi√≥n y podemos
# eliminarla:

# selecciona todas las filas y solo aquellas columnas 
# donde el valor en zeroVar  es FALSE. 
# solo conserva las columnas que no fueron identificadas como de baja varianza en el paso anterior
train <- train[, train_zero$zeroVar == 'FALSE']

# Como hicimos con la regresi√≥n lineal, para que la regresi√≥n log√≠stica tenga
# resultados significativos, es decir, que no se sobreajuste, es necesario reducir la cantidad de
# caracter√≠sticas de entrada. Podr√≠amos seguir adelante con una selecci√≥n gradual o similar,
# como hicimos en el tema de regresi√≥n lineal. Podr√≠amos implementar m√©todos de
# regularizaci√≥n de caracter√≠sticas. Sin embargo, queremos presentar un m√©todo de reducci√≥n
# de caracter√≠sticas univariadas utilizando el peso de la evidencia (WOE, Weight Of Evidence)
# y el valor de la informaci√≥n (IV, Information Value) y discutir c√≥mo podemos comprender
# c√≥mo usarlo en un problema de clasificaci√≥n junto con la regresi√≥n log√≠stica.


# Peso de la evidencia y valor de la informaci√≥n
# 
# Dada la posibilidad de cientos, incluso miles, de caracter√≠sticas posibles, tuvimos que
# aprender el uso de WOE y IV. Ahora bien, este m√©todo no es una panacea. En primer lugar,
# es univariado, por lo que las caracter√≠sticas que se descartan pueden volverse significativas
# en un modelo multivariado y viceversa. Podemos decir que proporciona un buen
# complemento a otros m√©todos y deber√≠as tenerlo en tu caja de herramientas de modelado.
# Creo que tuvo su origen en el mundo de la calificaci√≥n crediticia, por lo que, si trabajas en la
# industria financiera, es posible que ya est√©s familiarizado con √©l.
# Primero, veamos la f√≥rmula de WOE:
#   ùëäùëÇùê∏ = ln (porcentaje de eventos)/ (porcentaje de no eventos)

# El WOE sirve como componente del IV. Para funciones num√©ricas, agrupar√≠a tus datos y
# luego calcular√≠as WOE por separado para cada contenedor. Para los categ√≥ricos, o cuando
# est√°n codificados en caliente, agrupa para cada nivel y calcula el WOE por separado. 




# Tomemos un ejemplo y demostremos en R.
# Nuestros datos constan de una caracter√≠stica de entrada codificada como 0 o 1, por lo que
# solo tendremos dos contenedores. Para cada contenedor (bin), calculamos nuestro WOE. En
# el contenedor 1, o donde los valores son iguales a 0, hay cuatro observaciones como eventos
# y 96 como no eventos. Por el contrario, en el grupo 2, o donde los valores son iguales a 1,
# tenemos 12 observaciones como eventos y 88 como no eventos. Veamos c√≥mo calcular el
# WOE para cada contenedor:
bin1events <- 4
bin1nonEvents <- 96
bin2events <- 12
bin2nonEvents <- 88
totalEvents <- bin1events + bin2events
totalNonEvents <- bin1nonEvents + bin2nonEvents
# Now calculate the percentage per bin
bin1percentE <- bin1events / totalEvents
bin1percentNE <- bin1nonEvents / totalNonEvents
bin2percentE <- bin2events / totalEvents
bin2percentNE <- bin2nonEvents / totalNonEvents
# It's now possible to produce WOE
bin1WOE <- log(bin1percentE / bin1percentNE)
bin2WOE <- log(bin2percentE / bin2percentNE)
bin1WOE
bin2WOE

# La f√≥rmula es la siguiente:
#   ùêºùëâ = ‚àë(Porcentaje de eventos ‚àí Porcentaje de no eventos) ‚àó WOE


# Tomando nuestro ejemplo actual; esta es nuestra caracter√≠stica IV:
bin1IV <- (bin1percentE - bin1percentNE) * bin1WOE
bin2IV <- (bin2percentE - bin2percentNE) * bin2WOE

bin1IV + bin2IV

# El IV de la caracter√≠stica es 0.322. Ahora bien, ¬øqu√© significa eso? La respuesta corta es que
# depende. Se proporciona una heur√≠stica para ayudar a decidir qu√© umbral IV tiene sentido
# para su inclusi√≥n en el desarrollo del modelo:
#   ‚Ä¢ < 0.02 no predictivo
# ‚Ä¢ 0.02 a 0.1 d√©bil
# ‚Ä¢ 0.1 a 0.3 medio
# ‚Ä¢ 0.3 a 0.5 fuerte
# ‚Ä¢ 0.5 sospechoso
# Nuestro siguiente ejemplo nos proporcionar√° decisiones interesantes que tomar con respecto
# a d√≥nde trazar la l√≠nea.


# Selecci√≥n de caracter√≠sticas
# Lo que vamos a hacer ahora es usar el paquete Information para calcular los IV de nuestras
# funciones. Luego, se mostrar√° c√≥mo evaluar esos valores y tambi√©n ejecutar algunos gr√°ficos.
# Dado que no existen reglas estrictas y r√°pidas sobre los umbrales para la inclusi√≥n de
# funciones, daremos una opini√≥n sobre d√≥nde trazar la l√≠nea. Por supuesto, puedes rechazar
# eso y aplicar el tuyo propio.
# En este ejemplo, el c√≥digo crear√° una serie de tablas que puedes utilizar para explorar los
# resultados. Para comenzar, solo necesitas especificar los datos y la respuesta o variable "y":

IV <- Information::create_infotables(data = train, y = "y", parallel = FALSE)
# Esto nos dar√° un resumen IV de las 25 caracter√≠sticas principales:
knitr::kable(head(IV$Summary, 25))


# Los resultados nos muestran el n√∫mero de columna de la caracter√≠stica, el nombre de la
# caracter√≠stica y el IV.
# Observa que tenemos cinco caracter√≠sticas que posiblemente sean sospechosas. Estoy
# totalmente a favor de tomar cualquier caracter√≠stica con un IV superior a 0.02, que es la parte
# inferior de los predictores d√©biles. Eso nos dar√° 21 funciones de entrada. La caracter√≠stica V2
# es interesante. Si nos fijamos en los valores y pensamos en los datos, parece claro que se trata
# de la edad del cliente. Veamos c√≥mo se agrupan los datos, los valores WOE y los IV:

knitr::kable(IV$Tables$V2)

# Mira el contenedor n√∫mero 2, que creo que tiene una edad
# de cliente de 23 a√±os. Constituye casi el 27 por ciento del total de observaciones y aporta
# m√°s de la mitad del IV. ¬°Realmente sospechoso! ¬øC√≥mo ayudar√° cualquier algoritmo que
# produzcamos con estos datos si esta caracter√≠stica es AGE genuina, como sospecho? Sin
# embargo, eso est√° fuera del alcance de este esfuerzo y no vale la pena perder m√°s tiempo ni
# esfuerzo. Aqu√≠ podemos mostrar r√°pidamente un diagrama de barras de los WOE por
# contenedor:

Information::plot_infotables(IV, "V2", show_values = TRUE)

# Es interesante que exista una relaci√≥n algo lineal entre esta caracter√≠stica y la respuesta. Lo
# que se puede hacer es crear funciones que conviertan los valores agrupados en valores WOE.
# Estas nuevas funciones ser√≠an lineales y podr√≠an usarse en lugar de las funciones originales.
# Renunciaremos a eso porque ¬øqu√© m√©todo har√° eso por nosotros? As√≠ es, ¬°MARS
# puede hacer eso por nosotros! Aqu√≠ hay un diagrama de cuadr√≠cula de las
# cuatro caracter√≠sticas principales:
Information::plot_infotables(IV, IV$Summary$Variable[1:4], same_scales=TRUE)



# Ahora, dado el punto de corte que elegimos anteriormente, podemos seleccionar esas 21
# caracter√≠sticas:
features <- IV$Summary$Variable[1:21]
train_reduced <- train[, colnames(train) %in% features]
train_reduced$y <- train$y
# Ah√≠ tienes. Ahora estamos listos para comenzar a entrenar nuestro algoritmo.




# Validaci√≥n cruzada y regresi√≥n log√≠stica

# Nuestro objetivo aqu√≠ es construir un modelo utilizando validaci√≥n cruzada qu√≠ntuple.
# Utilizaremos el paquete de intercalaci√≥n para establecer nuestro esquema de muestreo y
# producir el modelo final. Comience por crear una funci√≥n trainControl() separada:
  
glm_control <-caret::trainControl(method = "cv",number = 5,returnResamp = "final")


# Este objeto se pasa como argumento para entrenar el algoritmo. Ahora producimos nuestras
# caracter√≠sticas de entrada, la variable de respuesta (debe ser un factor para que caret se entrene
# como regresi√≥n log√≠stica), configuramos nuestra semilla aleatoria y entrenamos el modelo.
# Para la funci√≥n train(), especificamos glm para el modelo lineal generalizado (GLM, Generalized Linear Model):
x <- train_reduced[, -22]
y <- as.factor(train_reduced$y)
set.seed(1988)
glm_fit <-caret::train(x, y, method = "glm",
                        trControl = glm_control,
                        trace = FALSE)
# Cuando hayas terminado, podr√°s comprobar r√°pidamente los resultados:
glm_fit$results




# Mira eso, ¬°96 por ciento de precisi√≥n! S√© que eso no tiene ning√∫n sentido porque si
# supusi√©ramos que todas las etiquetas en la respuesta eran cero, alcanzar√≠amos el 96 por
# ciento.



# Kappa se refiere a lo que se conoce como estad√≠stica Kappa de Cohen. La estad√≠stica Kappa
# proporciona una idea de este problema ajustando las puntuaciones de precisi√≥n, lo que se
# hace teniendo en cuenta que el modelo es completamente correcto por mera casualidad. La
# f√≥rmula de la estad√≠stica es la siguiente:
#   Kappa = (porcentaje de acuerdo ‚àí porcentaje de posible acuerdo)/(1
#                                                                    ‚àí porcentaje de posible acuerdo)
# El porcentaje de acuerdo es la tasa que el modelo acord√≥ para la clase (precisi√≥n) y el
# porcentaje de posible acuerdo de probabilidad es la tasa que el modelo acord√≥ aleatoriamente.
# Cuanto mayor sea la estad√≠stica, mejor ser√° el rendimiento, siendo el acuerdo m√°ximo uno.
# Entonces, con esta puntuaci√≥n Kappa, el modelo es pat√©tico.
# Bueno, Kappa ser√≠a √∫til con etiquetas m√°s equilibradas. Ahora nos quedamos para encontrar
# otras formas de examinar los resultados del modelo. Siempre es una buena idea comparar las
# distribuciones de probabilidad de las diferentes clases con un diagrama de densidad o caja.
# Aqu√≠ producimos una gr√°fica de densidad elegante y colorida en los datos de entrenamiento:

glm_train_pred <- predict(glm_fit, train, type = "prob")
colnames(glm_train_pred) <- c("zero", "one")
classifierplots::density_plot(train_reduced$y, glm_train_pred$one)


library(InformationValue)
glm_cutoff <-InformationValue::optimalCutoff(train_reduced$y,
    glm_train_pred$one,
    optimiseFor = 'Both',
    returnDiagnostics = TRUE)
# Si haces clic en glm_cutoff en tu entorno global o ejecutas View(glm_cutoff), ver√°s una lista
# de seis resultados diferentes:

# Si seleccionamos un l√≠mite de 0.0606, lograremos una tasa de verdaderos positivos (TPR) de
# casi el 61 por ciento. Sin embargo, m√°s del 19 por ciento ser√°n falsos positivos.
# Dado el desequilibrio en las clases, se trata de una enorme cantidad de clientes. Una matriz
# de confusi√≥n puede demostrar ese hecho:
InformationValue::confusionMatrix(train_reduced$y, glm_train_pred$one, threshold = 0.0607)



glm_test_pred <- predict(glm_fit, test, type = "prob")
colnames(glm_test_pred) <- c("zero", "one")
classifierplots::density_plot(test$y, glm_test_pred$one)

# Resultados muy similares en los datos de la prueba. ¬øQu√© pasa con una matriz de confusi√≥n
# dado nuestro umbral determinado durante el entrenamiento? Vamos a ver:
InformationValue::confusionMatrix(test$y, glm_test_pred$one, threshold = 0.0607)


# ¬°Resultados consistentes! Ahora, examinemos el rendimiento de este modelo en los datos de
# prueba para que podamos compararlo con lo que producir√° el pr√≥ximo modelo MARS. Dos
# m√©tricas deber√≠an abordar el problema: el √°rea bajo la curva (AUC, Area Under the Curve)
# y la p√©rdida logar√≠tmica (log-loss). El AUC proporciona un indicador √∫til de rendimiento y
# se puede demostrar que el AUC es igual a la probabilidad de que el observador identifique
# correctamente el caso positivo cuando se le presenta un par de casos elegidos al azar en los
# que un caso es positivo y el otro es positivo. En nuestro caso, cambiaremos de observador
# con nuestro algoritmo y evaluaremos en consecuencia. La p√©rdida logar√≠tmica es una m√©trica
# eficaz, ya que tiene en cuenta la probabilidad prevista y cu√°nto se desv√≠a de la etiqueta
# correcta. La siguiente f√≥rmula lo produce:
Metrics::auc(test$y, glm_test_pred$one)
Metrics::logLoss(test$y, glm_test_pred$one)
# Nuestras AUC no son tan buenas, dir√≠amos. Si el modelo no fuera mejor que una suposici√≥n
# aleatoria, entonces el AUC ser√≠a igual a 0.5 y, si fuera perfecto, ser√≠a 1. Nuestra p√©rdida
# logar√≠tmica solo es esencial cuando se compara con el siguiente modelo.


# Lo entrenaremos con validaci√≥n cruzada qu√≠ntuple y estableceremos nprune = 15 para limitar
# el n√∫mero m√°ximo de caracter√≠sticas a 15. Recuerda que, en regresi√≥n lineal m√∫ltiple, son
# posibles m√°s de 15 t√©rminos ya que se ajusta a splines por partes.
# Este c√≥digo nos dar√° nuestro objeto modelo. Ten en cuenta que esto puede tardar alg√∫n
# tiempo en completarse:
set.seed(1972)
earth_fit <-earth::earth(x = train[, -142],
    y = train[, 142],
    pmethod = 'cv',
    nfold = 5,
    degree = 1,
    minspan = -1,
    nprune = 15,
    glm = list(family = binomial)
    )
summary(earth_fit)
