geom_density()
ggplot(data=tayko_data, aes(tayko_data$log_Spending)) +
geom_histogram(aes(y =..density..), fill = "orange") +
geom_density()
hist(model$residuals, color = "grey")
plot(model)
plot(model)
plot(model)
View(data)
summary(data)
# Eliminar columnas con más del 50% de valores faltantes
threshold <- 50
library(readr)
data <- read_csv("Datos Alimentación.csv",locale = locale(encoding = "latin1"))
setwd("/Users/jonatanbadillo/Desktop/UNIVERSITY/7TH SEMESTER/MINERIA/Proyecto")
library(readr)
data <- read_csv("Datos Alimentación.csv",locale = locale(encoding = "latin1"))
View(data)
summary(data)
# Reemplaza "N/A" por NA en todas las columnas del dataframe
data[data == "N/A"] <- NA
# Cargar librerías necesarias
library(dplyr)
# Resumen de datos faltantes por columna
summary(is.na(data))
# Verifica la cantidad de valores faltantes por columna
colSums(is.na(data))
# Calcular el porcentaje de valores faltantes por columna
percent_missing <- colSums(is.na(data)) / nrow(data) * 100
print(percent_missing)
# Eliminar columnas con más del 50% de valores faltantes
threshold <- 50
data <- data[, percent_missing <= threshold]# se mantienen cols menores a 50%
# Verificar la estructura del dataframe después de eliminar columnas
colSums(is.na(data))
# Función para obtener la moda
# Define una función llamada get_mode que calcula la moda de un vector v.
# Identifica las columnas categóricas en el dataframe data.
# Itera sobre cada columna categórica.
# Para cada columna categórica, rellena los valores faltantes (NA) con la moda de esa columna
# utilizando la función get_mode.
get_mode <- function(v) {
uniqv <- unique(v)
uniqv[which.max(tabulate(match(v, uniqv)))]# encuentra el elemento único más frecuente en el vector v
}
# Imputar valores faltantes con la moda para las columnas categóricas restantes
categorical_columns <- sapply(data, is.character)
for (col in names(data)[categorical_columns]) {
data[[col]][is.na(data[[col]])] <- get_mode(data[[col]])
}
# Verificar nuevamente los valores faltantes
print(colSums(is.na(data)))
# Convertir la variable de respuesta a binaria
datos_dieta <- data %>%
mutate(
respuesta_binaria = ifelse(`¿Tus comidas están construidas con base en el plato del buen comer?` %in% c("Siempre", "Casi siempre"), "Sí", "No"),
respuesta_binaria = as.factor(respuesta_binaria)
)
# Dividir los datos en conjunto de entrenamiento y prueba
set.seed(123)  # Para reproducibilidad
indice_entrenamiento <- sample(1:nrow(datos_dieta), 0.8 * nrow(datos_dieta))
datos_entrenamiento <- datos_dieta[indice_entrenamiento, ]
datos_prueba <- datos_dieta[-indice_entrenamiento, ]
# Entrenar el modelo de regresión logística con la variable binaria
modelo_logistico_binario <- glm(respuesta_binaria ~ `¿Con qué frecuencia consumes alimentos ricos en calorías y bajos en nutrientes (comida chatarra)?` + `¿Las críticas sobre tu salud alimenticia influyen en tu alimentación?` + `¿Cómo afectan tus condiciones socioeconómicas y tu entorno cultural en tus hábitos alimenticios?` + `Tu alimentación cambia dependiendo de las personas que te rodean (familia, amigos, pareja, compañeros)`, data = datos_entrenamiento, family = binomial)
# Predicciones en el conjunto de prueba
predicciones_prob_binario <- predict(modelo_logistico_binario, newdata = datos_prueba, type = "response")
# Convierte las probabilidades predichas en etiquetas de clase binaria ("Sí" o "No") utilizando un umbral de probabilidad del 0.5.
predicciones_binario <- ifelse(predicciones_prob_binario > 0.5, "Sí", "No")
# Evaluar el rendimiento del modelo (por ejemplo, utilizando la matriz de confusión)
tabla_confusion_binario <- table(Predicted = predicciones_binario, Actual = datos_prueba$respuesta_binaria)
print(tabla_confusion_binario)
# Calcular la precisión del modelo
precision_binario <- sum(diag(tabla_confusion_binario)) / sum(tabla_confusion_binario)
print(paste("Precisión del modelo:", precision_binario))
# Resumen del modelo
summary(modelo_logistico_binario)
# Resumen del modelo
summary(modelo_logistico_binario)
model$coefficients
modelo_logistico_binario$coefficients
setwd("/Users/jonatanbadillo/Desktop/UNIVERSITY/Servicio-Social/Data-Mining/Ejemplos de Regresion Lineal/Tarea Regresion Lineal Multiple")
# Tarea de regresión lineal múltiple
# Predicción de beneficios de reventa de software
# Tayko Software es una empresa de catálogos de software que vende juegos y software
# educativo. Comenzó como un fabricante de software y luego agregó títulos de terceros a sus
# ofertas. Recientemente revisó su colección de artículos en un nuevo catálogo, que envió por
# correo a sus clientes. Este envío arrojó 2000 compras.
# Con base en estos datos, Tayko quiere crear un modelo para predecir la cantidad de gasto que
# producirá un cliente comprador. El archivo Tayko.csv contiene información sobre 2000
# compras. La siguiente tabla describe las variables que se utilizarán en el problema (el archivo
# de excel contiene variables adicionales).
# Descripción de variables para el ejemplo de software Tayko
# FREQ
# LAST_UPDATE
# WEB
# GENDER
# ADDRESS_RES
# ADDRESS_US
# SPENDING (response)
# Número de transacciones en el año anterior
# Número de días desde la última actualización del
# registro del cliente
# Si el cliente compró por pedido web al menos una vez
# Masculino o femenino
# Si es una dirección residencial
# Si es una dirección de EE. UU.
# Importe gastado por el cliente en envíos de prueba (edólares)
# a. Explora el monto del gasto creando una tabla dinámica para las variables categóricas
# y calcular el promedio y la desviación estándar del gasto en cada categoría.
# b. Explora la relación entre el gasto y cada uno de los dos continuos predictores
# mediante la creación de dos diagramas de dispersión (Spending vs Freq y Spending vs last_update_days_ago. ¿Parece haber una relación lineal?
#  c. Para ajustar un modelo predictivo de gastos:
#      i. Particiona los 2000 registros en conjuntos de entrenamiento y validación.
#      ii. Ejecuta un modelo de regresión lineal múltiple para Spending vs. los seis predictores. Dar la ecuación predictiva estimada.
#      iii. Con base en este modelo, ¿Qué tipo de comprador es más probable que gaste
#           una gran cantidad de dinero?
#      iv. Si usamos la eliminación hacia atrás para reducir el número de predictores,
#         que predictor se eliminaría primero del modelo?
#      v. Muestra cómo se calculan la predicción y el error de predicción para la primera compra en el conjunto de validación.
# Cargar y explorar los datos
# Primero, debemos cargar los datos y explorar las variables.
# Cargar las librerías necesarias
library(dplyr)
library(ggplot2)
library(caret)
# Cargar los datos
tayko_data <- read.csv("Tayko.csv")
# Visualizar los primeros registros del dataset
head(tayko_data)
# a. Crear una tabla dinámica para las variables categóricas
# Calcularemos el promedio y la desviación estándar del gasto (SPENDING) para cada categoría de las variables categóricas
# (WEB, GENDER, ADDRESS_RES, ADDRESS_US).
# Mostrar los nombres de las columnas
colnames(tayko_data)
# Crear una función para calcular promedio y desviación estándar
summary_stats <- function(data, var) {
data %>%
group_by(!!sym(var)) %>%
summarise(
mean_spending = mean(Spending, na.rm = TRUE),
sd_spending = sd(Spending, na.rm = TRUE)
)
}
# Aplicar la función a las variables categóricas
categorical_vars <- c("Web.order", "Gender.male", "Address_is_res", "US")
lapply(categorical_vars, summary_stats, data = tayko_data)
# b. Explorar la relación entre el gasto y cada uno de los predictores continuos
# Creamos diagramas de dispersión para Spending vs Freq y Spending vs LAST_UPDATE.
# Diagrama de dispersión para Spending vs Freq
ggplot(tayko_data, aes(x = Freq, y = Spending)) +
geom_point() +
geom_smooth(method = "lm", col = "red") +
labs(title = "Spending vs Freq", x = "Freq", y = "Spending")
# Diagrama de dispersión para Spending vs last_update_days_ago
ggplot(tayko_data, aes(x = last_update_days_ago, y = Spending)) +
geom_point() +
geom_smooth(method = "lm", col = "red") +
labs(title = "Spending vs last_update_days_ago", x = "last_update_days_ago", y = "Spending")
# c. Ajustar un modelo predictivo de gastos
# i. Particionar los datos en conjuntos de entrenamiento y validación
# Particionar los datos en conjuntos de entrenamiento y validación
library(caret)
library(lattice)
set.seed(123)
training_index <- createDataPartition(tayko_data$Spending, p = 0.7, list = FALSE)
training_data <- tayko_data[training_index, ]
validation_data <- tayko_data[-training_index, ]
# ii. Ejecutar un modelo de regresión lineal múltiple
# Ajustar el modelo de regresión lineal múltiple
model <- lm(Spending ~ Freq + last_update_days_ago + Web.order + Gender.male + Address_is_res + US, data = training_data)
summary(model)
# iii. Analizar el tipo de comprador que gasta más
# Mostrar los coeficientes del modelo
coefficients(model)
# iv. Eliminar predictores usando eliminación hacia atrás
# Eliminar predictores no significativos usando eliminación hacia atrás
step_model <- step(model, direction = "backward")
summary(step_model)
# v. Calcular predicción y error de predicción para la primera compra en el conjunto de validación
# Predicción para la primera compra en el conjunto de validación
first_pred <- predict(step_model, newdata = validation_data[1, ])
first_actual <- validation_data$Spending[1]
prediction_error <- first_actual - first_pred
# Mostrar predicción y error
list(prediction = first_pred, actual = first_actual, error = prediction_error)
# vi. Evaluar la precisión predictiva del modelo
# Evaluar el modelo en el conjunto de validación
predictions <- predict(step_model, newdata = validation_data)
validation_results <- data.frame(
actual = validation_data$Spending,
predicted = predictions,
residuals = validation_data$Spending - predictions
)
# Calcular el RMSE
rmse <- sqrt(mean(validation_results$residuals^2))
rmse
# vii. Crear un histograma de los residuos del modelo
# Crear histograma de los residuos
ggplot(validation_results, aes(x = residuals)) +
geom_histogram(binwidth = 5, fill = "blue", color = "black") +
labs(title = "Histograma de los Residuos", x = "Residuos", y = "Frecuencia")
# Evaluar si los residuos siguen una distribución normal
qqnorm(validation_results$residuals)
qqline(validation_results$residuals, col = "red")
shapiro.test(model$residuals)
plot(model,5)
# Distancia de cook
# ningun valor es influyente
# Esto es generalmente una situación favorable, ya que significa que el modelo
# es robusto a la presencia de valores atípicos o influyentes.
cooks.distance(model)
which(cooks.distance(model) > 1)
# ----------------------- alternativa -------------------
# Transformación logarítmica de la variable de respuesta Spending
tayko_data$log_Spending <- log(tayko_data$Spending + 1) # Agregar 1 para evitar log(0)
# Particionar los datos en conjuntos de entrenamiento y validación nuevamente
set.seed(123)
training_index <- createDataPartition(tayko_data$log_Spending, p = 0.7, list = FALSE)
training_data <- tayko_data[training_index, ]
validation_data <- tayko_data[-training_index, ]
# Ajustar el modelo de regresión lineal múltiple con la variable transformada
model_log <- lm(log_Spending ~ Freq + last_update_days_ago + Web.order + Gender.male + Address_is_res + US, data = training_data)
summary(model_log)
# Evaluar el modelo en el conjunto de validación
log_predictions <- predict(model_log, newdata = validation_data)
validation_results_log <- data.frame(
actual = validation_data$log_Spending,
predicted = log_predictions,
residuals = validation_data$log_Spending - log_predictions
)
# Calcular el RMSE para el modelo transformado
rmse_log <- sqrt(mean(validation_results_log$residuals^2))
rmse_log
# Crear QQ-plot para los residuos del modelo transformado
qqnorm(validation_results_log$residuals)
qqline(validation_results_log$residuals, col = "red")
# Crear histograma de los residuos del modelo transformado
ggplot(validation_results_log, aes(x = residuals)) +
geom_histogram(binwidth = 0.1, fill = "blue", color = "black") +
labs(title = "Histograma de los Residuos (Modelo Transformado)", x = "Residuos", y = "Frecuencia")
# Calcular el RMSE para el modelo transformado
rmse_log <- sqrt(mean(validation_results_log$residuals^2))
rmse_log
hist(model$residuals, color = "grey")s
plot(model)
plot(model_log)
plot(model_log)
plot(model)
santander <- read.csv("santander_prepd.csv")
# 2. Aquí cargamos el archivo, luego verificamos las dimensiones y examinamos una
# tabla de etiquetas de clientes:
library(readr)
santander <- read_csv("Desktop/UNIVERSITY/Servicio-Social/Data-Mining/Regresion Logistica/santander_prepd.csv")
dim(santander)
table(santander$y)
View(santander)
trainIndex <- caret::createDataPartition(santander$y, p = 0.8, list = FALSE)
train <- santander[trainIndex, ]
test <- santander[-trainIndex, ]
table(train$y)
table(test$y)
# identifica las características en el conjunto de datos train que tienen una varianza muy baja.
# Las características con una varianza muy baja pueden no ser informativas para su modelo y podrían eliminarse antes del entrenamiento.
train_zero <- caret::nearZeroVar(train, saveMetrics = TRUE)
View(train_zero)
table(train_zero$zeroVar)
# 6. Bien, una característica ahora tiene variación cero debido a la división y podemos
# eliminarla:
train <- train[, train_zero$zeroVar == 'FALSE']
# selecciona todas las filas y solo aquellas columnas
# donde el valor en zeroVar  es FALSE.
# solo conserva las columnas que no fueron identificadas como de baja varianza en el paso anterior
train <- train[, train_zero$zeroVar == 'FALSE']
library(tidyverse)
library(magrittr)
library(caret)
library(classifierplots)
install.packages("classifierplots")
# magrittr en nuestro entorno:
library(tidyverse)
library(magrittr)
library(caret)
library(classifierplots)
library(earth)
install.packages("earth")
library(tidyverse)
library(magrittr)
library(caret)
library(classifierplots)
library(earth)
library(Information)
install.packages("Information")
install.packages("Metrics")
library(tidyverse)
library(magrittr)
library(caret)
library(classifierplots)
library(earth)
library(Information)
library(Metrics)
# 2. Aquí cargamos el archivo, luego verificamos las dimensiones y examinamos una
# tabla de etiquetas de clientes:
library(readr)
santander <- read_csv("Desktop/UNIVERSITY/Servicio-Social/Data-Mining/Regresion Logistica/santander_prepd.csv")
dim(santander)
table(santander$y)
set.seed(1966)
trainIndex <- caret::createDataPartition(santander$y, p = 0.8, list = FALSE)
train <- santander[trainIndex, ]
test <- santander[-trainIndex, ]
# 4. Veamos cómo se equilibra la respuesta entre los dos conjuntos de datos:
table(train$y)
table(test$y)
# identifica las características en el conjunto de datos train que tienen una varianza muy baja.
# Las características con una varianza muy baja pueden no ser informativas para su modelo y podrían eliminarse antes del entrenamiento.
train_zero <- caret::nearZeroVar(train, saveMetrics = TRUE)
table(train_zero$zeroVar)
# selecciona todas las filas y solo aquellas columnas
# donde el valor en zeroVar  es FALSE.
# solo conserva las columnas que no fueron identificadas como de baja varianza en el paso anterior
train <- train[, train_zero$zeroVar == 'FALSE']
# Tomemos un ejemplo y demostremos en R.
# Nuestros datos constan de una característica de entrada codificada como 0 o 1, por lo que
# solo tendremos dos contenedores. Para cada contenedor (bin), calculamos nuestro WOE. En
# el contenedor 1, o donde los valores son iguales a 0, hay cuatro observaciones como eventos
# y 96 como no eventos. Por el contrario, en el grupo 2, o donde los valores son iguales a 1,
# tenemos 12 observaciones como eventos y 88 como no eventos. Veamos cómo calcular el
# WOE para cada contenedor:
bin1events <- 4
bin1nonEvents <- 96
bin2events <- 12
bin2nonEvents <- 88
totalEvents <- bin1events + bin2events
totalNonEvents <- bin1nonEvents + bin2nonEvents
# Now calculate the percentage per bin
bin1percentE <- bin1events / totalEvents
bin1percentNE <- bin1nonEvents / totalNonEvents
bin2percentE <- bin2events / totalEvents
bin2percentNE <- bin2nonEvents / totalNonEvents
# It's now possible to produce WOE
bin1WOE <- log(bin1percentE / bin1percentNE)
bin2WOE <- log(bin2percentE / bin2percentNE)
> bin1WOE
bin1WOE
bin2WOE
bin1IV <- (bin1percentE - bin1percentNE) * bin1WOE
bin2IV <- (bin2percentE - bin2percentNE) * bin2WOE
bin1IV + bin2IV
IV <- Information::create_infotables(data = train, y = "y", parallel = FALSE)
IV <- Information::create_infotables(data = train, y = "y", parallel = FALSE)
# Esto nos dará un resumen IV de las 25 características principales:
knitr::kable(head(IV$Summary, 25))
View(train)
# Los resultados nos muestran el número de columna de la característica, el nombre de la
# característica y el IV.
# Observa que tenemos cinco características que posiblemente sean sospechosas. Estoy
# totalmente a favor de tomar cualquier característica con un IV superior a 0.02, que es la parte
# inferior de los predictores débiles. Eso nos dará 21 funciones de entrada. La característica V2
# es interesante. Si nos fijamos en los valores y pensamos en los datos, parece claro que se trata
# de la edad del cliente. Veamos cómo se agrupan los datos, los valores WOE y los IV:
knitr::kable(IV$Tables$V2)
Information::plot_infotables(IV, "V2", show_values = TRUE)
# Es interesante que exista una relación algo lineal entre esta característica y la respuesta. Lo
# que se puede hacer es crear funciones que conviertan los valores agrupados en valores WOE.
# Estas nuevas funciones serían lineales y podrían usarse en lugar de las funciones originales.
# Renunciaremos a eso porque ¿qué método hará eso por nosotros? Así es, ¡MARS
# puede hacer eso por nosotros! Aquí hay un diagrama de cuadrícula de las
# cuatro características principales:
Information::plot_infotables(IV, IV$Summary$Variable[1:4], same_scales=TRUE)
features <- IV$Summary$Variable[1:21]
train_reduced <- train[, colnames(train) %in% features]
train_reduced$y <- train$y
glm_control <-caret::trainControl(method = "cv",number = 5,returnResamp = "final")
x <- train_reduced[, -22]
y <- as.factor(train_reduced$y)
set.seed(1988)
glm_fit <-caret::train(x, y, method = "glm",
trControl = glm_control,
trace = FALSE)
Cuando hayas terminado, podrás comprobar rápidamente los resultados:
# Cuando hayas terminado, podrás comprobar rápidamente los resultados:
glm_fit$results
glm_train_pred <- predict(glm_fit, train, type = "prob")
colnames(glm_train_pred) <- c("zero", "one")
classifierplots::density_plot(train_reduced$y, glm_train_pred$one)
glm_train_pred <- predict(glm_fit, train, type = "prob")
colnames(glm_train_pred) <- c("zero", "one")
classifierplots::density_plot(train_reduced$y, glm_train_pred$one)
# Dado el desequilibrio en las clases, se trata de una enorme cantidad de clientes. Una matriz
# de confusión puede demostrar ese hecho:
InformationValue::confusionMatrix(train_reduced$y, glm_train_pred$one, threshold = 0.0607)
install.packages("Information")
install.packages("Information")
library(InformationValue)
install.packages("InformationValue")
library(InformationValue)
install.packages("~/Downloads/InformationValue/inst/doc/InformationValue.R", repos = NULL)
glm_test_pred <- predict(glm_fit, test, type = "prob")
colnames(glm_test_pred) <- c("zero", "one")
classifierplots::density_plot(test$y, glm_test_pred$one)
# Resultados muy similares en los datos de la prueba. ¿Qué pasa con una matriz de confusión
# dado nuestro umbral determinado durante el entrenamiento? Vamos a ver:
InformationValue::confusionMatrix(test$y, glm_test_pred$one, threshold = 0.0607)
Metrics::auc(test$y, glm_test_pred$one)
Metrics::logLoss(test$y, glm_test_pred$one)
set.seed(1972)
earth_fit <-earth::earth(x = train[, -142],
y = train[, 142],
pmethod = 'cv',
nfold = 5,
degree = 1,
minspan = -1,
nprune = 15,
glm = list(family = binomial)
)
summary(earth_fit)
# Ahora, especificamos la validación cruzada, pero earth simultáneamente selecciona y elimina
# características hacia adelante y hacia atrás, utilizando la validación cruzada generalizada
# (GCV, Generalized Cross-Validation). Por lo tanto, los resultados de GCV y rss para una
# característica se normalizan de 0 a 100 con fines de comparación.
# Como hicimos anteriormente, una gráfica de las densidades de probabilidad es útil y earth
# viene con su propia función plotd():
earth::plotd(earth_fit)
pred <- predict(earth_fit, train, type = 'response')
mars_cutoff <-InformationValue::optimalCutoff(
train$y,
pred,
optimiseFor = 'Both',
returnDiagnostics = TRUE)
install.packages("forecast")
library(forecast)
set.seed(1966)
ar1 <- arima.sim(list(order = c(1, 0, 0), ar = 0.5), n = 200)
forecast::autoplot(ar1, main = "AR1")
install.packages("forecast")
# Ahora, examinemos ACF:
forecast::autoplot(acf(ar1, plot = F), main = "ACF of simulated AR1")
forecast::autoplot(pacf(ar1, plot = F), main = "PACF of simulated AR1")
set.seed(123)
ma1 <- arima.sim(list(order = c(0, 0, 1), ma = -0.5), n = 200)
forecast::autoplot(ma1, main = "MA1")
# La figura anterior es el gráfico ACF y ahora veremos el gráfico PACF:
forecast::autoplot(pacf(ma1, plot = F), main = "PACF of simulated MA1")
setwd("~/Desktop/UNIVERSITY/Servicio-Social/Data-Mining/Clustering a los datos")
library(readr)
data <- read_csv("Datos/NINFAS/NINFAS-2023-limpiado.csv")
library(readr)
data <- read_csv("~/Desktop/UNIVERSITY/Servicio-Social/Data-Mining/Datos/NINFAS/NINFAS-2023-limpiado.csv")
View(data)
summary(data)
data
print(n = 60)
# las columnas que representan variables numéricas sean de tipo numérico
data$O3 <- as.numeric(data$O3)
data$NO2 <- as.numeric(data$NO2)
data$CO <- as.numeric(data$CO)
data$SO2 <- as.numeric(data$SO2)
data$`PM-10` <- as.numeric(data$PM-10)
# las columnas que representan variables numéricas sean de tipo numérico
data$O3 <- as.numeric(data$O3)
data$NO2 <- as.numeric(data$NO2)
data$CO <- as.numeric(data$CO)
data$SO2 <- as.numeric(data$SO2)
data$PM-10 <- as.numeric(data$PM-10)
# las columnas que representan variables numéricas sean de tipo numérico
data$O3 <- as.numeric(data$O3)
data$NO2 <- as.numeric(data$NO2)
data$CO <- as.numeric(data$CO)
data$SO2 <- as.numeric(data$SO2)
data$`PM-10` <- as.numeric(data$`PM-10`)
data$`PM-2.5` <- as.numeric(data$`PM-2.5`)
# Revisar los primeros datos para confirmar que están bien cargados
head(data)
# Normalizar las columnas numéricas
data_scaled <- scale(data[, c("O3", "NO2", "CO", "SO2", "`PM-10`", "`PM-2.5`")])
# Normalizar las columnas numéricas
data_scaled <- scale(data[, c("O3", "NO2", "CO", "SO2", "PM-10", "PM-2.5")])
# Método del codo para determinar el número óptimo de clusters
set.seed(123)
wss <- (nrow(data_scaled)-1)*sum(apply(data_scaled, 2, var))
for (i in 2:15) wss[i] <- sum(kmeans(data_scaled, centers=i)$withinss)
# Graficar el método del codo
plot(1:15, wss, type="b", xlab="Número de Clusters", ylab="Suma de Cuadrados Dentro de los Clusters (WSS)")
# Aplicar k-means con 4 clusters
set.seed(123) # Para reproducibilidad
kmeans_result <- kmeans(data_scaled, centers=4)
# Agregar la asignación de clusters a los datos originales
data$cluster <- kmeans_result$cluster
# Ver los resultados
head(data)
View(data)
# visualización
library(ggplot2)
# Crear un gráfico de pares coloreado por clusters
pairs(data[, c("O3", "NO2", "CO", "SO2", "`PM-10`", "`PM-2.5`")], col=data$cluster, pch=19)
install.packages("ggplot2")
# visualización
library(ggplot2)
# Crear un gráfico de pares coloreado por clusters
pairs(data[, c("O3", "NO2", "CO", "SO2", "`PM-10`", "`PM-2.5`")], col=data$cluster, pch=19)
# Crear un gráfico de pares coloreado por clusters
pairs(data[, c("O3", "NO2", "CO", "SO2", "PM-10", "PM-2.5")], col=data$cluster, pch=19)
# Calcular la media de cada variable para cada cluster
aggregate(data[, c("O3", "NO2", "CO", "SO2", "PM-10", "PM-2.5")], by=list(cluster=data$cluster), FUN=mean)
plot(data)
# Calcular la media de cada variable para cada cluster
aggregate(data[, c("O3", "NO2", "CO", "SO2", "PM-10", "PM-2.5")], by=list(cluster=data$cluster), FUN=mean)
# Crear un gráfico de pares coloreado por clusters
pairs(data[, c("O3", "NO2", "CO", "SO2", "PM-10", "PM-2.5")], col=data$cluster, pch=19)
# Calcular la media de cada variable para cada cluster
aggregate(data[, c("O3", "NO2", "CO", "SO2", "PM-10", "PM-2.5")], by=list(cluster=data$cluster), FUN=mean)
